{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ab5618-1210-4dd8-901d-6a129bc03352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def file_exists(filename):\n",
    "    return os.path.exists(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efcce589-2b4d-4742-852e-098013304975",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"arguana\"\n",
    "# prompt_name = \"prompt_method2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25eb48c-9232-4fa0-b512-5d4b28ecc6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a helpful assistant. A user wants to find a passage that satisfies a particular requirement from a query. You are shown the query so that you understand this particular requirement’s context, but your objective is to focus solely on the requirement and the passage. Based on the passage, you will determine whether the requirement is met. \n",
    "\n",
    "Here is the query: {query}\n",
    "\n",
    "Here is the requirement extracted from the query:  {req}\n",
    "\n",
    "Here is the passage:  {passage}\n",
    "\n",
    "Explain whether the passage satisfies the requirement.\n",
    "Think step by step, showing all of your reasoning.  Output a single score on a scale of 0-5. A lower score would mean the passage is less likely to satisfy the requirement, whereas a higher score would mean the passage is more likely to satisfy the requirement. I need to parse your answer in a certain format, so in the end, only output the score after the key SCORE:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040ab165-fdd6-44a7-9edb-0cbc5f6c656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. A user wants to find a passage that satisfies a particular requirement from a query. You are shown the query so that you understand this particular requirement’s context, but your objective is to focus solely on the requirement and the passage. Based on the passage, you will determine whether the requirement is met. \n",
      "\n",
      "Here is the query: {query}\n",
      "\n",
      "Here is the requirement extracted from the query:  {req}\n",
      "\n",
      "Here is the passage:  {passage}\n",
      "\n",
      "Explain whether the passage satisfies the requirement.\n",
      "Think step by step, showing all of your reasoning.  Output a single score on a scale of 0-5. A lower score would mean the passage is less likely to satisfy the requirement, whereas a higher score would mean the passage is more likely to satisfy the requirement. I need to parse your answer in a certain format, so in the end, only output the score after the key SCORE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7593661-c03f-4021-bbc8-199eebfcbcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"./{dataset_name}/{prompt_name}.pickle\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    raise RuntimeError(f\"{filename} exists!!!\")\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(prompt_template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e97b644-0cd6-4a94-ab36-4f81605c86fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98c8de61-930a-4d12-a631-863b2520e2d0",
   "metadata": {},
   "source": [
    "# RankGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d6d9a19-42c1-4cdf-9863-b33a00f53a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "dataset_name = \"clinic-trial\"\n",
    "prompt_name = \"prompt_rankGPT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31946afe-ab4e-43e0-964d-122a14bedaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = {'prev': \"\", 'post': \"\"}\n",
    "prompt_template['prev'] = \"\"\"Task Description: The motivation of the Information Retrieval task is that clinical trials are experiments conducted in the development of new medical treatments, drugs or devices, and recruiting candidates for a trial is often a time-consuming and resource-intensive effort. You will see a query and a list of passages. A query is a patient case report (either in the form of electronic patient records or ad-hoc queries). A passage is a clinical trial description. This Information Retrieval task is to improve patient recruitment for clinical trials. The overall goal of this specific information retrieval IR task is to match eligible patients (the query) to clinical trials (the passage) for recruitment. You are RankGPT, an intelligent assistant that can rank passages based on their relevancy to the query.\"\"\"\n",
    "\n",
    "prompt_template['post'] = \"\"\"Search Query: {query_text}.\n",
    "\n",
    "Task Description: The motivation of the Information Retrieval task is that clinical trials are experiments conducted in the development of new medical treatments, drugs or devices, and recruiting candidates for a trial is often a time-consuming and resource-intensive effort. You will see a query and a list of passages. A query is a patient case report (either in the form of electronic patient records or ad-hoc queries). A passage is a clinical trial description. This Information Retrieval task is to improve patient recruitment for clinical trials. The overall goal of this specific information retrieval IR task is to match eligible patients (the query) to clinical trials (the passage) for recruitment.\n",
    "\n",
    "Rank the {length} passages above based on their relevance to the search query. The passages\n",
    "should be listed in descending order using identifiers, and the most relevant passages should be\n",
    "listed first, and the output format should be [] > [], e.g., [1] > [2]. Only response the ranking results,\n",
    "do not say any word or explain.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9c313d7-dbb4-4127-80dc-9ddc1099290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev\n",
      "Task Description: The motivation of the Information Retrieval task is that clinical trials are experiments conducted in the development of new medical treatments, drugs or devices, and recruiting candidates for a trial is often a time-consuming and resource-intensive effort. You will see a query and a list of passages. A query is a patient case report (either in the form of electronic patient records or ad-hoc queries). A passage is a clinical trial description. This Information Retrieval task is to improve patient recruitment for clinical trials. The overall goal of this specific information retrieval IR task is to match eligible patients (the query) to clinical trials (the passage) for recruitment. You are RankGPT, an intelligent assistant that can rank passages based on their relevancy to the query.\n",
      "==================================================\n",
      "post\n",
      "Search Query: {query_text}.\n",
      "\n",
      "Task Description: The motivation of the Information Retrieval task is that clinical trials are experiments conducted in the development of new medical treatments, drugs or devices, and recruiting candidates for a trial is often a time-consuming and resource-intensive effort. You will see a query and a list of passages. A query is a patient case report (either in the form of electronic patient records or ad-hoc queries). A passage is a clinical trial description. This Information Retrieval task is to improve patient recruitment for clinical trials. The overall goal of this specific information retrieval IR task is to match eligible patients (the query) to clinical trials (the passage) for recruitment.\n",
      "\n",
      "Rank the {length} passages above based on their relevance to the search query. The passages\n",
      "should be listed in descending order using identifiers, and the most relevant passages should be\n",
      "listed first, and the output format should be [] > [], e.g., [1] > [2]. Only response the ranking results,\n",
      "do not say any word or explain.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# print(prompt_template)\n",
    "\n",
    "for k in prompt_template:\n",
    "    print(k)\n",
    "    print(prompt_template[k])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "405cd272-881d-4b38-83a6-46bfcf7cfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"./{dataset_name}/{prompt_name}.pickle\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    raise RuntimeError(f\"{filename} exists!!!\")\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(prompt_template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c2536d-9990-4459-ad8d-d62a21f4535f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3c489-ecb4-4903-ad30-b970e2194b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dn_correct",
   "language": "python",
   "name": "dn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
