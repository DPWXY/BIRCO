{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Leveraging powerful deep learning techniques, the end-to-end (E2E) learning\nof communication system is able to outperform the classical communication\nsystem. Unfortunately, this communication system cannot be trained by deep\nlearning without known channel. To deal with this problem, a generative\nadversarial network (GAN) based training scheme has been recently proposed to\nimitate the real channel. However, the gradient vanishing and overfitting\nproblems of GAN will result in the serious performance degradation of E2E\nlearning of communication system. To mitigate these two problems, we propose a\nresidual aided GAN (RA-GAN) based training scheme in this paper. Particularly,\ninspired by the idea of residual learning, we propose a residual generator to\nmitigate the gradient vanishing problem by realizing a more robust gradient\nbackpropagation. Moreover, to cope with the overfitting problem, we reconstruct\nthe loss function for training by adding a regularizer, which limits the\nrepresentation ability of RA-GAN. Simulation results show that the trained\nresidual generator has better generation performance than the conventional\ngenerator, and the proposed RA-GAN based training scheme can achieve the\nnear-optimal block error rate (BLER) performance with a negligible\ncomputational complexity increase in both the theoretical channel model and the\nray-tracing based channel dataset."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Training deep neural networks on large datasets containing high-dimensional\ndata requires a large amount of computation. A solution to this problem is\ndata-parallel distributed training, where a model is replicated into several\ncomputational nodes that have access to different chunks of the data. This\napproach, however, entails high communication rates and latency because of the\ncomputed gradients that need to be shared among nodes at every iteration. The\nproblem becomes more pronounced in the case that there is wireless\ncommunication between the nodes (i.e. due to the limited network bandwidth). To\naddress this problem, various compression methods have been proposed including\nsparsification, quantization, and entropy encoding of the gradients. Existing\nmethods leverage the intra-node information redundancy, that is, they compress\ngradients at each node independently. In contrast, we advocate that the\ngradients across the nodes are correlated and propose methods to leverage this\ninter-node redundancy to improve compression efficiency. Depending on the node\ncommunication protocol (parameter server or ring-allreduce), we propose two\ninstances of the LGC approach that we coin Learned Gradient Compression (LGC).\nOur methods exploit an autoencoder (i.e. trained during the first stages of the\ndistributed training) to capture the common information that exists in the\ngradients of the distributed nodes. We have tested our LGC methods on the image\nclassification and semantic segmentation tasks using different convolutional\nneural networks (ResNet50, ResNet101, PSPNet) and multiple datasets (ImageNet,\nCifar10, CamVid). The ResNet101 model trained for image classification on\nCifar10 achieved an accuracy of 93.57%, which is lower than the baseline\ndistributed training with uncompressed gradients only by 0.18%."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Training large machine learning models requires a distributed computing\napproach, with communication of the model updates being the bottleneck. For\nthis reason, several methods based on the compression (e.g., sparsification\nand/or quantization) of updates were recently proposed, including QSGD\n(Alistarh et al., 2017), TernGrad (Wen et al., 2017), SignSGD (Bernstein et\nal., 2018), and DQGD (Khirirat et al., 2018). However, none of these methods\nare able to learn the gradients, which renders them incapable of converging to\nthe true optimum in the batch mode, incompatible with non-smooth regularizers,\nand slows down their convergence. In this work we propose a new distributed\nlearning method --- DIANA --- which resolves these issues via compression of\ngradient differences. We perform a theoretical analysis in the strongly convex\nand nonconvex settings and show that our rates are superior to existing rates.\nOur analysis of block-quantization and differences between $\\ell_2$ and\n$\\ell_\\infty$ quantization closes the gaps in theory and practice. Finally, by\napplying our analysis technique to TernGrad, we establish the first convergence\nrate for this method."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Communication overhead is a major bottleneck hampering the scalability of\ndistributed machine learning systems. Recently, there has been a surge of\ninterest in using gradient compression to improve the communication efficiency\nof distributed neural network training. Using 1-bit quantization, signSGD with\nmajority vote achieves a 32x reduction on communication cost. However, its\nconvergence is based on unrealistic assumptions and can diverge in practice. In\nthis paper, we propose a general distributed compressed SGD with Nesterov's\nmomentum. We consider two-way compression, which compresses the gradients both\nto and from workers. Convergence analysis on nonconvex problems for general\ngradient compressors is provided. By partitioning the gradient into blocks, a\nblockwise compressor is introduced such that each gradient block is compressed\nand transmitted in 1-bit format with a scaling factor, leading to a nearly 32x\nreduction on communication. Experimental results show that the proposed method\nconverges as fast as full-precision distributed momentum SGD and achieves the\nsame testing accuracy. In particular, on distributed ResNet training with 7\nworkers on the ImageNet, the proposed algorithm achieves the same testing\naccuracy as momentum SGD using full-precision gradients, but with $46\\%$ less\nwall clock time."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "We consider machine learning applications that train a model by leveraging\ndata distributed over a trusted network, where communication constraints can\ncreate a performance bottleneck. A number of recent approaches propose to\novercome this bottleneck through compression of gradient updates. However, as\nmodels become larger, so does the size of the gradient updates. In this paper,\nwe propose an alternate approach to learn from distributed data that quantizes\ndata instead of gradients, and can support learning over applications where the\nsize of gradient updates is prohibitive. Our approach leverages the dependency\nof the computed gradient on data samples, which lie in a much smaller space in\norder to perform the quantization in the smaller dimension data space. At the\ncost of an extra gradient computation, the gradient estimate can be refined by\nconveying the difference between the gradient at the quantized data point and\nthe original gradient using a small number of bits. Lastly, in order to save\ncommunication, our approach adds a layer that decides whether to transmit a\nquantized data sample or not based on its importance for learning. We analyze\nthe convergence of the proposed approach for smooth convex and non-convex\nobjective functions and show that we can achieve order optimal convergence\nrates with communication that mostly depends on the data rather than the model\n(gradient) dimension. We use our proposed algorithm to train ResNet models on\nthe CIFAR-10 and ImageNet datasets, and show that we can achieve an order of\nmagnitude savings over gradient compression methods. These communication\nsavings come at the cost of increasing computation at the learning agent, and\nthus our approach is beneficial in scenarios where communication load is the\nmain problem."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "The proliferation of big data has brought an urgent demand for\nprivacy-preserving data publishing. Traditional solutions to this demand have\nlimitations on effectively balancing the tradeoff between privacy and utility\nof the released data. Thus, the database community and machine learning\ncommunity have recently studied a new problem of relational data synthesis\nusing generative adversarial networks (GAN) and proposed various algorithms.\nHowever, these algorithms are not compared under the same framework and thus it\nis hard for practitioners to understand GAN's benefits and limitations. To\nbridge the gaps, we conduct so far the most comprehensive experimental study\nthat investigates applying GAN to relational data synthesis. We introduce a\nunified GAN-based framework and define a space of design solutions for each\ncomponent in the framework, including neural network architectures and training\nstrategies. We conduct extensive experiments to explore the design space and\ncompare with traditional data synthesis approaches. Through extensive\nexperiments, we find that GAN is very promising for relational data synthesis,\nand provide guidance for selecting appropriate design solutions. We also point\nout limitations of GAN and identify future research directions."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Large-scale distributed training requires significant communication bandwidth\nfor gradient exchange that limits the scalability of multi-node training, and\nrequires expensive high-bandwidth network infrastructure. The situation gets\neven worse with distributed training on mobile devices (federated learning),\nwhich suffers from higher latency, lower throughput, and intermittent poor\nconnections. In this paper, we find 99.9% of the gradient exchange in\ndistributed SGD is redundant, and propose Deep Gradient Compression (DGC) to\ngreatly reduce the communication bandwidth. To preserve accuracy during\ncompression, DGC employs four methods: momentum correction, local gradient\nclipping, momentum factor masking, and warm-up training. We have applied Deep\nGradient Compression to image classification, speech recognition, and language\nmodeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and\nLibrispeech Corpus. On these scenarios, Deep Gradient Compression achieves a\ngradient compression ratio from 270x to 600x without losing accuracy, cutting\nthe gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from\n488MB to 0.74MB. Deep gradient compression enables large-scale distributed\ntraining on inexpensive commodity 1Gbps Ethernet and facilitates distributed\ntraining on mobile. Code is available at:\nhttps://github.com/synxlin/deep-gradient-compression."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Distributed training is an effective way to accelerate the training process\nof large-scale deep learning models. However, the parameter exchange and\nsynchronization of distributed stochastic gradient descent introduce a large\namount of communication overhead. Gradient compression is an effective method\nto reduce communication overhead. In synchronization SGD compression methods,\nmany Top-k sparsification based gradient compression methods have been proposed\nto reduce the communication. However, the centralized method based on the\nparameter servers has the single point of failure problem and limited\nscalability, while the decentralized method with global parameter exchanging\nmay reduce the convergence rate of training. In contrast with Top-$k$ based\nmethods, we proposed a gradient compression method with globe gradient vector\nsketching, which uses the Count-Sketch structure to store the gradients to\nreduce the loss of the accuracy in the training process, named global-sketching\nSGD (gs-SGD). The gs-SGD has better convergence efficiency on deep learning\nmodels and a communication complexity of O($\\log d*\\log P$), where $d$ is the\nnumber of model parameters and P is the number of workers. We conducted\nexperiments on GPU clusters to verify that our method has better convergence\nefficiency than global Top-$k$ and Sketching-based methods. In addition, gs-SGD\nachieves 1.3-3.1x higher throughput compared with gTop-$k$, and 1.1-1.2x higher\nthroughput compared with original Sketched-SGD."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Conditional Generative Adversarial Networks (cGANs) have enabled controllable\nimage synthesis for many vision and graphics applications. However, recent\ncGANs are 1-2 orders of magnitude more compute-intensive than modern\nrecognition CNNs. For example, GauGAN consumes 281G MACs per image, compared to\n0.44G MACs for MobileNet-v3, making it difficult for interactive deployment. In\nthis work, we propose a general-purpose compression framework for reducing the\ninference time and model size of the generator in cGANs. Directly applying\nexisting compression methods yields poor performance due to the difficulty of\nGAN training and the differences in generator architectures. We address these\nchallenges in two ways. First, to stabilize GAN training, we transfer knowledge\nof multiple intermediate representations of the original model to its\ncompressed model and unify unpaired and paired learning. Second, instead of\nreusing existing CNN designs, our method finds efficient architectures via\nneural architecture search. To accelerate the search process, we decouple the\nmodel training and search via weight sharing. Experiments demonstrate the\neffectiveness of our method across different supervision settings, network\narchitectures, and learning methods. Without losing image quality, we reduce\nthe computation of CycleGAN by 21x, Pix2pix by 12x, MUNIT by 29x, and GauGAN by\n9x, paving the way for interactive image synthesis."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Generative adversarial networks (GANs) have been recently adopted for\nsuper-resolution, an application closely related to what is referred to as\n\"downscaling\" in the atmospheric sciences: improving the spatial resolution of\nlow-resolution images. The ability of conditional GANs to generate an ensemble\nof solutions for a given input lends itself naturally to stochastic\ndownscaling, but the stochastic nature of GANs is not usually considered in\nsuper-resolution applications. Here, we introduce a recurrent, stochastic\nsuper-resolution GAN that can generate ensembles of time-evolving\nhigh-resolution atmospheric fields for an input consisting of a low-resolution\nsequence of images of the same field. We test the GAN using two datasets, one\nconsisting of radar-measured precipitation from Switzerland, the other of cloud\noptical thickness derived from the Geostationary Earth Observing Satellite 16\n(GOES-16). We find that the GAN can generate realistic, temporally consistent\nsuper-resolution sequences for both datasets. The statistical properties of the\ngenerated ensemble are analyzed using rank statistics, a method adapted from\nensemble weather forecasting; these analyses indicate that the GAN produces\nclose to the correct amount of variability in its outputs. As the GAN generator\nis fully convolutional, it can be applied after training to input images larger\nthan the images used to train it. It is also able to generate time series much\nlonger than the training sequences, as demonstrated by applying the generator\nto a three-month dataset of the precipitation radar data. The source code to\nour GAN is available at https://github.com/jleinonen/downscaling-rnn-gan."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Recent years have witnessed the rapid progress of generative adversarial\nnetworks (GANs). However, the success of the GAN models hinges on a large\namount of training data. This work proposes a regularization approach for\ntraining robust GAN models on limited data. We theoretically show a connection\nbetween the regularized loss and an f-divergence called LeCam-divergence, which\nwe find is more robust under limited training data. Extensive experiments on\nseveral benchmark datasets demonstrate that the proposed regularization scheme\n1) improves the generalization performance and stabilizes the learning dynamics\nof GAN models under limited training data, and 2) complements the recent data\naugmentation methods. These properties facilitate training GAN models to\nachieve state-of-the-art performance when only limited training data of the\nImageNet benchmark is available."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "A standard approach in large scale machine learning is distributed stochastic\ngradient training, which requires the computation of aggregated stochastic\ngradients over multiple nodes on a network. Communication is a major bottleneck\nin such applications, and in recent years, compressed stochastic gradient\nmethods such as QSGD (quantized SGD) and sparse SGD have been proposed to\nreduce communication. It was also shown that error compensation can be combined\nwith compression to achieve better convergence in a scheme that each node\ncompresses its local stochastic gradient and broadcast the result to all other\nnodes over the network in a single pass. However, such a single pass broadcast\napproach is not realistic in many practical implementations. For example, under\nthe popular parameter server model for distributed learning, the worker nodes\nneed to send the compressed local gradients to the parameter server, which\nperforms the aggregation. The parameter server has to compress the aggregated\nstochastic gradient again before sending it back to the worker nodes. In this\nwork, we provide a detailed analysis on this two-pass communication model and\nits asynchronous parallel variant, with error-compensated compression both on\nthe worker nodes and on the parameter server. We show that the\nerror-compensated stochastic gradient algorithm admits three very nice\nproperties: 1) it is compatible with an \\emph{arbitrary} compression technique;\n2) it admits an improved convergence rate than the non error-compensated\nstochastic gradient methods such as QSGD and sparse SGD; 3) it admits linear\nspeedup with respect to the number of workers. The empirical study is also\nconducted to validate our theoretical results."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Neural networks have proven their capabilities by outperforming many other\napproaches on regression or classification tasks on various kinds of data.\nOther astonishing results have been achieved using neural nets as data\ngenerators, especially in settings of generative adversarial networks (GANs).\nOne special application is the field of image domain translations. Here, the\ngoal is to take an image with a certain style (e.g. a photography) and\ntransform it into another one (e.g. a painting). If such a task is performed\nfor unpaired training examples, the corresponding GAN setting is complex, the\nneural networks are large, and this leads to a high peak memory consumption\nduring, both, training and evaluation phase. This sets a limit to the highest\nprocessable image size. We address this issue by the idea of not processing the\nwhole image at once, but to train and evaluate the domain translation on the\nlevel of overlapping image subsamples. This new approach not only enables us to\ntranslate high-resolution images that otherwise cannot be processed by the\nneural network at once, but also allows us to work with comparably small neural\nnetworks and with limited hardware resources. Additionally, the number of\nimages required for the training process is significantly reduced. We present\nhigh-quality results on images with a total resolution of up to over 50\nmegapixels and emonstrate that our method helps to preserve local image details\nwhile it also keeps global consistency."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Generative Adversarial Networks (GAN) have many potential medical imaging\napplications, including data augmentation, domain adaptation, and model\nexplanation. Due to the limited memory of Graphical Processing Units (GPUs),\nmost current 3D GAN models are trained on low-resolution medical images, these\nmodels either cannot scale to high-resolution or are prone to patchy artifacts.\nIn this work, we propose a novel end-to-end GAN architecture that can generate\nhigh-resolution 3D images. We achieve this goal by using different\nconfigurations between training and inference. During training, we adopt a\nhierarchical structure that simultaneously generates a low-resolution version\nof the image and a randomly selected sub-volume of the high-resolution image.\nThe hierarchical design has two advantages: First, the memory demand for\ntraining on high-resolution images is amortized among sub-volumes. Furthermore,\nanchoring the high-resolution sub-volumes to a single low-resolution image\nensures anatomical consistency between sub-volumes. During inference, our model\ncan directly generate full high-resolution images. We also incorporate an\nencoder with a similar hierarchical structure into the model to extract\nfeatures from the images. Experiments on 3D thorax CT and brain MRI demonstrate\nthat our approach outperforms state of the art in image generation. We also\ndemonstrate clinical applications of the proposed model in data augmentation\nand clinical-relevant feature extraction."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "As deep learning is showing unprecedented success in medical image analysis\ntasks, the lack of sufficient medical data is emerging as a critical problem.\nWhile recent attempts to solve the limited data problem using Generative\nAdversarial Networks (GAN) have been successful in generating realistic images\nwith diversity, most of them are based on image-to-image translation and thus\nrequire extensive datasets from different domains. Here, we propose a novel\nmodel that can successfully generate 3D brain MRI data from random vectors by\nlearning the data distribution. Our 3D GAN model solves both image blurriness\nand mode collapse problems by leveraging alpha-GAN that combines the advantages\nof Variational Auto-Encoder (VAE) and GAN with an additional code discriminator\nnetwork. We also use the Wasserstein GAN with Gradient Penalty (WGAN-GP) loss\nto lower the training instability. To demonstrate the effectiveness of our\nmodel, we generate new images of normal brain MRI and show that our model\noutperforms baseline models in both quantitative and qualitative measurements.\nWe also train the model to synthesize brain disorder MRI data to demonstrate\nthe wide applicability of our model. Our results suggest that the proposed\nmodel can successfully generate various types and modalities of 3D whole brain\nvolumes from a small set of training data."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Parallel implementations of stochastic gradient descent (SGD) have received\nsignificant research attention, thanks to excellent scalability properties of\nthis algorithm, and to its efficiency in the context of training deep neural\nnetworks. A fundamental barrier for parallelizing large-scale SGD is the fact\nthat the cost of communicating the gradient updates between nodes can be very\nlarge. Consequently, lossy compression heuristics have been proposed, by which\nnodes only communicate quantized gradients. Although effective in practice,\nthese heuristics do not always provably converge, and it is not clear whether\nthey are optimal.\n  In this paper, we propose Quantized SGD (QSGD), a family of compression\nschemes which allow the compression of gradient updates at each node, while\nguaranteeing convergence under standard assumptions. QSGD allows the user to\ntrade off compression and convergence time: it can communicate a sublinear\nnumber of bits per iteration in the model dimension, and can achieve\nasymptotically optimal communication cost. We complement our theoretical\nresults with empirical data, showing that QSGD can significantly reduce\ncommunication cost, while being competitive with standard uncompressed\ntechniques on a variety of real tasks.\n  In particular, experiments show that gradient quantization applied to\ntraining of deep neural networks for image classification and automated speech\nrecognition can lead to significant reductions in communication cost, and\nend-to-end training time. For instance, on 16 GPUs, we are able to train a\nResNet-152 network on ImageNet 1.8x faster to full accuracy. Of note, we show\nthat there exist generic parameter settings under which all known network\narchitectures preserve or slightly improve their full accuracy when using\nquantization."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "The Generative Models have gained considerable attention in the field of\nunsupervised learning via a new and practical framework called Generative\nAdversarial Networks (GAN) due to its outstanding data generation capability.\nMany models of GAN have proposed, and several practical applications emerged in\nvarious domains of computer vision and machine learning. Despite GAN's\nexcellent success, there are still obstacles to stable training. The problems\nare due to Nash-equilibrium, internal covariate shift, mode collapse, vanishing\ngradient, and lack of proper evaluation metrics. Therefore, stable training is\na crucial issue in different applications for the success of GAN. Herein, we\nsurvey several training solutions proposed by different researchers to\nstabilize GAN training. We survey, (I) the original GAN model and its modified\nclassical versions, (II) detail analysis of various GAN applications in\ndifferent domains, (III) detail study about the various GAN training obstacles\nas well as training solutions. Finally, we discuss several new issues as well\nas research outlines to the topic."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Deep Neural Networks (DNNs) have achieved im- pressive accuracy in many\napplication domains including im- age classification. Training of DNNs is an\nextremely compute- intensive process and is solved using variants of the\nstochastic gradient descent (SGD) algorithm. A lot of recent research has\nfocussed on improving the performance of DNN training. In this paper, we\npresent optimization techniques to improve the performance of the data parallel\nsynchronous SGD algorithm using the Torch framework: (i) we maintain data\nin-memory to avoid file I/O overheads, (ii) we present a multi-color based MPI\nAllreduce algorithm to minimize communication overheads, and (iii) we propose\noptimizations to the Torch data parallel table framework that handles\nmulti-threading. We evaluate the performance of our optimizations on a Power 8\nMinsky cluster with 32 nodes and 128 NVidia Pascal P100 GPUs. With our\noptimizations, we are able to train 90 epochs of the ResNet-50 model on the\nImagenet-1k dataset using 256 GPUs in just 48 minutes. This significantly\nimproves on the previously best known performance of training 90 epochs of the\nResNet-50 model on the same dataset using 256 GPUs in 65 minutes. To the best\nof our knowledge, this is the best known training performance demonstrated for\nthe Imagenet- 1k dataset."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Sentiment analysis is a task that may suffer from a lack of data in certain\ncases, as the datasets are often generated and annotated by humans. In cases\nwhere data is inadequate for training discriminative models, generate models\nmay aid training via data augmentation. Generative Adversarial Networks (GANs)\nare one such model that has advanced the state of the art in several tasks,\nincluding as image and text generation. In this paper, I train GAN models on\nlow resource datasets, then use them for the purpose of data augmentation\ntowards improving sentiment classifier generalization. Given the constraints of\nlimited data, I explore various techniques to train the GAN models. I also\npresent an analysis of the quality of generated GAN data as more training data\nfor the GAN is made available. In this analysis, the generated data is\nevaluated as a test set (against a model trained on real data points) as well\nas a training set to train classification models. Finally, I also conduct a\nvisual analysis by projecting the generated and the real data into a\ntwo-dimensional space using the t-Distributed Stochastic Neighbor Embedding\n(t-SNE) method."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Many engineering problems require the prediction of\nrealization-to-realization variability or a refined description of modeled\nquantities. In that case, it is necessary to sample elements from unknown\nhigh-dimensional spaces with possibly millions of degrees of freedom. While\nthere exist methods able to sample elements from probability density functions\n(PDF) with known shapes, several approximations need to be made when the\ndistribution is unknown. In this paper the sampling method, as well as the\ninference of the underlying distribution, are both handled with a data-driven\nmethod known as generative adversarial networks (GAN), which trains two\ncompeting neural networks to produce a network that can effectively generate\nsamples from the training set distribution. In practice, it is often necessary\nto draw samples from conditional distributions. When the conditional variables\nare continuous, only one (if any) data point corresponding to a particular\nvalue of a conditioning variable may be available, which is not sufficient to\nestimate the conditional distribution. This work handles this problem using an\na priori estimation of the conditional moments of a PDF. Two approaches,\nstochastic estimation, and an external neural network are compared here for\ncomputing these moments; however, any preferred method can be used. The\nalgorithm is demonstrated in the case of the deconvolution of a filtered\nturbulent flow field. It is shown that all the versions of the proposed\nalgorithm effectively sample the target conditional distribution with minimal\nimpact on the quality of the samples compared to state-of-the-art methods.\nAdditionally, the procedure can be used as a metric for the diversity of\nsamples generated by a conditional GAN (cGAN) conditioned with continuous\nvariables."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Due to the substantial computational cost, training state-of-the-art deep\nneural networks for large-scale datasets often requires distributed training\nusing multiple computation workers. However, by nature, workers need to\nfrequently communicate gradients, causing severe bottlenecks, especially on\nlower bandwidth connections. A few methods have been proposed to compress\ngradient for efficient communication, but they either suffer a low compression\nratio or significantly harm the resulting model accuracy, particularly when\napplied to convolutional neural networks. To address these issues, we propose a\nmethod to reduce the communication overhead of distributed deep learning. Our\nkey observation is that gradient updates can be delayed until an unambiguous\n(high amplitude, low variance) gradient has been calculated. We also present an\nefficient algorithm to compute the variance with negligible additional cost. We\nexperimentally show that our method can achieve very high compression ratio\nwhile maintaining the result model accuracy. We also analyze the efficiency\nusing computation and communication cost models and provide the evidence that\nthis method enables distributed deep learning for many scenarios with commodity\nenvironments."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "In this paper, we present a communication-efficient federated learning\nframework inspired by quantized compressed sensing. The presented framework\nconsists of gradient compression for wireless devices and gradient\nreconstruction for a parameter server (PS). Our strategy for gradient\ncompression is to sequentially perform block sparsification, dimensional\nreduction, and quantization. Thanks to gradient sparsification and\nquantization, our strategy can achieve a higher compression ratio than one-bit\ngradient compression. For accurate aggregation of the local gradients from the\ncompressed signals at the PS, we put forth an approximate minimum mean square\nerror (MMSE) approach for gradient reconstruction using the\nexpectation-maximization generalized-approximate-message-passing (EM-GAMP)\nalgorithm. Assuming Bernoulli Gaussian-mixture prior, this algorithm\niteratively updates the posterior mean and variance of local gradients from the\ncompressed signals. We also present a low-complexity approach for the gradient\nreconstruction. In this approach, we use the Bussgang theorem to aggregate\nlocal gradients from the compressed signals, then compute an approximate MMSE\nestimate of the aggregated gradient using the EM-GAMP algorithm. We also\nprovide a convergence rate analysis of the presented framework. Using the MNIST\ndataset, we demonstrate that the presented framework achieves almost identical\nperformance with the case that performs no compression, while significantly\nreducing communication overhead for federated learning."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Recently there has been a surge of research on improving the communication\nefficiency of distributed training. However, little work has been done to\nsystematically understand whether the network is the bottleneck and to what\nextent.\n  In this paper, we take a first-principles approach to measure and analyze the\nnetwork performance of distributed training. As expected, our measurement\nconfirms that communication is the component that blocks distributed training\nfrom linear scale-out. However, contrary to the common belief, we find that the\nnetwork is running at low utilization and that if the network can be fully\nutilized, distributed training can achieve a scaling factor of close to one.\nMoreover, while many recent proposals on gradient compression advocate over\n100x compression ratio, we show that under full network utilization, there is\nno need for gradient compression in 100 Gbps network. On the other hand, a\nlower speed network like 10 Gbps requires only 2x--5x gradients compression\nratio to achieve almost linear scale-out. Compared to application-level\ntechniques like gradient compression, network-level optimizations do not\nrequire changes to applications and do not hurt the performance of trained\nmodels. As such, we advocate that the real challenge of distributed training is\nfor the network community to develop high-performance network transport to\nfully utilize the network capacity and achieve linear scale-out."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Communication overhead severely hinders the scalability of distributed\nmachine learning systems. Recently, there has been a growing interest in using\ngradient compression to reduce the communication overhead of the distributed\ntraining. However, there is little understanding of applying gradient\ncompression to adaptive gradient methods. Moreover, its performance benefits\nare often limited by the non-negligible compression overhead. In this paper, we\nfirst introduce a novel adaptive gradient method with gradient compression. We\nshow that the proposed method has a convergence rate of\n$\\mathcal{O}(1/\\sqrt{T})$ for non-convex problems. In addition, we develop a\nscalable system called BytePS-Compress for two-way compression, where the\ngradients are compressed in both directions between workers and parameter\nservers. BytePS-Compress pipelines the compression and decompression on CPUs\nand achieves a high degree of parallelism. Empirical evaluations show that we\nimprove the training time of ResNet50, VGG16, and BERT-base by 5.0%, 58.1%,\n23.3%, respectively, without any accuracy loss with 25 Gb/s networking.\nFurthermore, for training the BERT models, we achieve a compression rate of\n333x compared to the mixed-precision training."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "We present a learned image compression system based on GANs, operating at\nextremely low bitrates. Our proposed framework combines an encoder,\ndecoder/generator and a multi-scale discriminator, which we train jointly for a\ngenerative learned compression objective. The model synthesizes details it\ncannot afford to store, obtaining visually pleasing results at bitrates where\nprevious methods fail and show strong artifacts. Furthermore, if a semantic\nlabel map of the original image is available, our method can fully synthesize\nunimportant regions in the decoded image such as streets and trees from the\nlabel map, proportionally reducing the storage cost. A user study confirms that\nfor low bitrates, our approach is preferred to state-of-the-art methods, even\nwhen they use more than double the bits."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Distributed stochastic gradient descent (SGD) approach has been widely used\nin large-scale deep learning, and the gradient collective method is vital to\nensure the training scalability of the distributed deep learning system.\nCollective communication such as AllReduce has been widely adopted for the\ndistributed SGD process to reduce the communication time. However, AllReduce\nincurs large bandwidth resources while most gradients are sparse in many cases\nsince many gradient values are zeros and should be efficiently compressed for\nbandwidth saving. To reduce the sparse gradient communication overhead, we\npropose Sparse-Sketch Reducer (S2 Reducer), a novel sketch-based sparse\ngradient aggregation method with convergence guarantees. S2 Reducer reduces the\ncommunication cost by only compressing the non-zero gradients with count-sketch\nand bitmap, and enables the efficient AllReduce operators for parallel SGD\ntraining. We perform extensive evaluation against four state-of-the-art methods\nover five training models. Our results show that S2 Reducer converges to the\nsame accuracy, reduces 81\\% sparse communication overhead, and achieves 1.8$\n\\times $ speedup compared to state-of-the-art approaches."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Training large neural networks requires distributing learning across multiple\nworkers, where the cost of communicating gradients can be a significant\nbottleneck. signSGD alleviates this problem by transmitting just the sign of\neach minibatch stochastic gradient. We prove that it can get the best of both\nworlds: compressed gradients and SGD-level convergence rate. The relative\n$\\ell_1/\\ell_2$ geometry of gradients, noise and curvature informs whether\nsignSGD or SGD is theoretically better suited to a particular problem. On the\npractical side we find that the momentum counterpart of signSGD is able to\nmatch the accuracy and convergence speed of Adam on deep Imagenet models. We\nextend our theory to the distributed setting, where the parameter server uses\nmajority vote to aggregate gradient signs from each worker enabling 1-bit\ncompression of worker-server communication in both directions. Using a theorem\nby Gauss we prove that majority vote can achieve the same reduction in variance\nas full precision distributed SGD. Thus, there is great promise for sign-based\noptimisation schemes to achieve fast communication and fast convergence. Code\nto reproduce experiments is to be found at https://github.com/jxbz/signSGD ."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "The Gradient Boosted Tree (GBT) algorithm is one of the most popular machine\nlearning algorithms used in production, for tasks that include Click-Through\nRate (CTR) prediction and learning-to-rank. To deal with the massive datasets\navailable today, many distributed GBT methods have been proposed. However, they\nall assume a row-distributed dataset, addressing scalability only with respect\nto the number of data points and not the number of features, and increasing\ncommunication cost for high-dimensional data. In order to allow for scalability\nacross both the data point and feature dimensions, and reduce communication\ncost, we propose block-distributed GBTs. We achieve communication efficiency by\nmaking full use of the data sparsity and adapting the Quickscorer algorithm to\nthe block-distributed setting. We evaluate our approach using datasets with\nmillions of features, and demonstrate that we are able to achieve multiple\norders of magnitude reduction in communication cost for sparse data, with no\nloss in accuracy, while providing a more scalable design. As a result, we are\nable to reduce the training time for high-dimensional data, and allow more\ncost-effective scale-out without the need for expensive network communication."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "While the availability of large datasets is perceived to be a key requirement\nfor training deep neural networks, it is possible to train such models with\nrelatively little data. However, compensating for the absence of large datasets\ndemands a series of actions to enhance the quality of the existing samples and\nto generate new ones. This paper summarizes our winning submission to the\n\"Data-Centric AI\" competition. We discuss some of the challenges that arise\nwhile training with a small dataset, offer a principled approach for systematic\ndata quality enhancement, and propose a GAN-based solution for synthesizing new\ndata points. Our evaluations indicate that the dataset generated by the\nproposed pipeline offers 5% accuracy improvement while being significantly\nsmaller than the baseline."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Neural network-based methods for abstractive summarization produce outputs\nthat are more fluent than other techniques, but which can be poor at content\nselection. This work proposes a simple technique for addressing this issue: use\na data-efficient content selector to over-determine phrases in a source\ndocument that should be part of the summary. We use this selector as a\nbottom-up attention step to constrain the model to likely phrases. We show that\nthis approach improves the ability to compress text, while still generating\nfluent summaries. This two-step process is both simpler and higher performing\nthan other end-to-end content selection models, leading to significant\nimprovements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the\ncontent selector can be trained with as little as 1,000 sentences, making it\neasy to transfer a trained summarizer to a new domain."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Although the distributed machine learning methods can speed up the training\nof large deep neural networks, the communication cost has become the\nnon-negligible bottleneck to constrain the performance. To address this\nchallenge, the gradient compression based communication-efficient distributed\nlearning methods were designed to reduce the communication cost, and more\nrecently the local error feedback was incorporated to compensate for the\ncorresponding performance loss. However, in this paper, we will show that a new\n\"gradient mismatch\" problem is raised by the local error feedback in\ncentralized distributed training and can lead to degraded performance compared\nwith full-precision training. To solve this critical problem, we propose two\nnovel techniques, 1) step ahead and 2) error averaging, with rigorous\ntheoretical analysis. Both our theoretical and empirical results show that our\nnew methods can handle the \"gradient mismatch\" problem. The experimental\nresults show that we can even train faster with common gradient compression\nschemes than both the full-precision training and local error feedback\nregarding the training epochs and without performance loss."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Large amount of data is often required to train and deploy useful machine\nlearning models in industry. Smaller enterprises do not have the luxury of\naccessing enough data for machine learning, For privacy sensitive fields such\nas banking, insurance and healthcare, aggregating data to a data warehouse\nposes a challenge of data security and limited computational resources. These\nchallenges are critical when developing machine learning algorithms in\nindustry. Several attempts have been made to address the above challenges by\nusing distributed learning techniques such as federated learning over disparate\ndata stores in order to circumvent the need for centralised data aggregation.\nThis paper proposes an improved algorithm to securely train deep neural\nnetworks over several data sources in a distributed way, in order to eliminate\nthe need to centrally aggregate the data and the need to share the data thus\npreserving privacy. The proposed method allows training of deep neural networks\nusing data from multiple de-linked nodes in a distributed environment and to\nsecure the representation shared during training. Only a representation of the\ntrained models (network architecture and weights) are shared. The algorithm was\nevaluated on existing healthcare patients data and the performance of this\nimplementation was compared to that of a regular deep neural network trained on\na single centralised architecture. This algorithm will pave a way for\ndistributed training of neural networks on privacy sensitive applications where\nraw data may not be shared directly or centrally aggregating this data in a\ndata warehouse is not feasible."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Sufficient supervised information is crucial for any machine learning models\nto boost performance. However, labeling data is expensive and sometimes\ndifficult to obtain. Active learning is an approach to acquire annotations for\ndata from a human oracle by selecting informative samples with a high\nprobability to enhance performance. In recent emerging studies, a generative\nadversarial network (GAN) has been integrated with active learning to generate\ngood candidates to be presented to the oracle. In this paper, we propose a\nnovel model that is able to obtain labels for data in a cheaper manner without\nthe need to query an oracle. In the model, a novel reward for each sample is\ndevised to measure the degree of uncertainty, which is obtained from a\nclassifier trained with existing labeled data. This reward is used to guide a\nconditional GAN to generate informative samples with a higher probability for a\ncertain label. With extensive evaluations, we have confirmed the effectiveness\nof the model, showing that the generated samples are capable of improving the\nclassification performance in popular image classification tasks."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Generative adversarial networks (GANs) are very popular to generate realistic\nimages, but they often suffer from the training instability issues and the\nphenomenon of mode loss. In order to attain greater diversity in GAN\nsynthesized data, it is critical to solving the problem of mode loss. Our work\nexplores probabilistic approaches to GAN modelling that could allow us to\ntackle these issues. We present Prb-GANs, a new variation that uses dropout to\ncreate a distribution over the network parameters with the posterior learnt\nusing variational inference. We describe theoretically and validate\nexperimentally using simple and complex datasets the benefits of such an\napproach. We look into further improvements using the concept of uncertainty\nmeasures. Through a set of further modifications to the loss functions for each\nnetwork of the GAN, we are able to get results that show the improvement of GAN\nperformance. Our methods are extremely simple and require very little\nmodification to existing GAN architecture."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "While pre-trained language models (e.g., BERT) have achieved impressive\nresults on different natural language processing tasks, they have large numbers\nof parameters and suffer from big computational and memory costs, which make\nthem difficult for real-world deployment. Therefore, model compression is\nnecessary to reduce the computation and memory cost of pre-trained models. In\nthis work, we aim to compress BERT and address the following two challenging\npractical issues: (1) The compression algorithm should be able to output\nmultiple compressed models with different sizes and latencies, in order to\nsupport devices with different memory and latency limitations; (2) The\nalgorithm should be downstream task agnostic, so that the compressed models are\ngenerally applicable for different downstream tasks. We leverage techniques in\nneural architecture search (NAS) and propose NAS-BERT, an efficient method for\nBERT compression. NAS-BERT trains a big supernet on a search space containing a\nvariety of architectures and outputs multiple compressed models with adaptive\nsizes and latency. Furthermore, the training of NAS-BERT is conducted on\nstandard self-supervised pre-training tasks (e.g., masked language model) and\ndoes not depend on specific downstream tasks. Thus, the compressed models can\nbe used across various downstream tasks. The technical challenge of NAS-BERT is\nthat training a big supernet on the pre-training task is extremely costly. We\nemploy several techniques including block-wise search, search space pruning,\nand performance approximation to improve search efficiency and accuracy.\nExtensive experiments on GLUE and SQuAD benchmark datasets demonstrate that\nNAS-BERT can find lightweight models with better accuracy than previous\napproaches, and can be directly applied to different downstream tasks with\nadaptive model sizes for different requirements of memory or latency."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Large-scale distributed training of Deep Neural Networks (DNNs) on\nstate-of-the-art platforms is expected to be severely communication\nconstrained. To overcome this limitation, numerous gradient compression\ntechniques have been proposed and have demonstrated high compression ratios.\nHowever, most existing methods do not scale well to large scale distributed\nsystems (due to gradient build-up) and/or fail to evaluate model fidelity (test\naccuracy) on large datasets. To mitigate these issues, we propose a new\ncompression technique, Scalable Sparsified Gradient Compression (ScaleCom),\nthat leverages similarity in the gradient distribution amongst learners to\nprovide significantly improved scalability. Using theoretical analysis, we show\nthat ScaleCom provides favorable convergence guarantees and is compatible with\ngradient all-reduce techniques. Furthermore, we experimentally demonstrate that\nScaleCom has small overheads, directly reduces gradient traffic and provides\nhigh compression rates (65-400X) and excellent scalability (up to 64 learners\nand 8-12X larger batch sizes over standard training) across a wide range of\napplications (image, language, and speech) without significant accuracy loss."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Large-scale distributed optimization is of great importance in various\napplications. For data-parallel based distributed learning, the inter-node\ngradient communication often becomes the performance bottleneck. In this paper,\nwe propose the error compensated quantized stochastic gradient descent\nalgorithm to improve the training efficiency. Local gradients are quantized to\nreduce the communication overhead, and accumulated quantization error is\nutilized to speed up the convergence. Furthermore, we present theoretical\nanalysis on the convergence behaviour, and demonstrate its advantage over\ncompetitors. Extensive experiments indicate that our algorithm can compress\ngradients by a factor of up to two magnitudes without performance degradation."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "With the rapid increase of big data, distributed Machine Learning (ML) has\nbeen widely applied in training large-scale models. Stochastic Gradient Descent\n(SGD) is arguably the workhorse algorithm of ML. Distributed ML models trained\nby SGD involve large amounts of gradient communication, which limits the\nscalability of distributed ML. Thus, it is important to compress the gradients\nfor reducing communication. In this paper, we propose FastSGD, a Fast\ncompressed SGD framework for distributed ML. To achieve a high compression\nratio at a low cost, FastSGD represents the gradients as key-value pairs, and\ncompresses both the gradient keys and values in linear time complexity. For the\ngradient value compression, FastSGD first uses a reciprocal mapper to transform\noriginal values into reciprocal values, and then, it utilizes a logarithm\nquantization to further reduce reciprocal values to small integers. Finally,\nFastSGD filters reduced gradient integers by a given threshold. For the\ngradient key compression, FastSGD provides an adaptive fine-grained delta\nencoding method to store gradient keys with fewer bits. Extensive experiments\non practical ML models and datasets demonstrate that FastSGD achieves the\ncompression ratio up to 4 orders of magnitude, and accelerates the convergence\ntime up to 8x, compared with state-of-the-art methods."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "A large portion of data mining and analytic services use modern machine\nlearning techniques, such as deep learning. The state-of-the-art results by\ndeep learning come at the price of an intensive use of computing resources. The\nleading frameworks (e.g., TensorFlow) are executed on GPUs or on high-end\nservers in datacenters. On the other end, there is a proliferation of personal\ndevices with possibly free CPU cycles; this can enable services to run in\nusers' homes, embedding machine learning operations. In this paper, we ask the\nfollowing question: Is distributed deep learning computation on WAN connected\ndevices feasible, in spite of the traffic caused by learning tasks? We show\nthat such a setup rises some important challenges, most notably the ingress\ntraffic that the servers hosting the up-to-date model have to sustain.\n  In order to reduce this stress, we propose adaComp, a novel algorithm for\ncompressing worker updates to the model on the server. Applicable to stochastic\ngradient descent based approaches, it combines efficient gradient selection and\nlearning rate modulation. We then experiment and measure the impact of\ncompression, device heterogeneity and reliability on the accuracy of learned\nmodels, with an emulator platform that embeds TensorFlow into Linux containers.\nWe report a reduction of the total amount of data sent by workers to the server\nby two order of magnitude (e.g., 191-fold reduction for a convolutional network\non the MNIST dataset), when compared to a standard asynchronous stochastic\ngradient descent, while preserving model accuracy."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Generative models are known to be difficult to assess. Recent works,\nespecially on generative adversarial networks (GANs), produce good visual\nsamples of varied categories of images. However, the validation of their\nquality is still difficult to define and there is no existing agreement on the\nbest evaluation process. This paper aims at making a step toward an objective\nevaluation process for generative models. It presents a new method to assess a\ntrained generative model by evaluating the test accuracy of a classifier\ntrained with generated data. The test set is composed of real images.\nTherefore, The classifier accuracy is used as a proxy to evaluate if the\ngenerative model fit the true data distribution. By comparing results with\ndifferent generated datasets we are able to classify and compare generative\nmodels. The motivation of this approach is also to evaluate if generative\nmodels can help discriminative neural networks to learn, i.e., measure if\ntraining on generated data is able to make a model successful at testing on\nreal settings. Our experiments compare different generators from the\nVariational Auto-Encoders (VAE) and Generative Adversarial Network (GAN)\nframeworks on MNIST and fashion MNIST datasets. Our results show that none of\nthe generative models is able to replace completely true data to train a\ndiscriminative model. But they also show that the initial GAN and WGAN are the\nbest choices to generate on MNIST database (Modified National Institute of\nStandards and Technology database) and fashion MNIST database."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Generative Adversarial Networks (GANs) have received a great deal of\nattention due in part to recent success in generating original, high-quality\nsamples from visual domains. However, most current methods only allow for users\nto guide this image generation process through limited interactions. In this\nwork we develop a novel GAN framework that allows humans to be \"in-the-loop\" of\nthe image generation process. Our technique iteratively accepts relative\nconstraints of the form \"Generate an image more like image A than image B\".\nAfter each constraint is given, the user is presented with new outputs from the\nGAN, informing the next round of feedback. This feedback is used to constrain\nthe output of the GAN with respect to an underlying semantic space that can be\ndesigned to model a variety of different notions of similarity (e.g. classes,\nattributes, object relationships, color, etc.). In our experiments, we show\nthat our GAN framework is able to generate images that are of comparable\nquality to equivalent unsupervised GANs while satisfying a large number of the\nconstraints provided by users, effectively changing a GAN into one that allows\nusers interactive control over image generation without sacrificing image\nquality."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "The performance and efficiency of distributed training of Deep Neural\nNetworks highly depend on the performance of gradient averaging among all\nparticipating nodes, which is bounded by the communication between nodes. There\nare two major strategies to reduce communication overhead: one is to hide\ncommunication by overlapping it with computation, and the other is to reduce\nmessage sizes. The first solution works well for linear neural architectures,\nbut latest networks such as ResNet and Inception offer limited opportunity for\nthis overlapping. Therefore, researchers have paid more attention to minimizing\ncommunication. In this paper, we present a novel gradient compression framework\nderived from insights of real gradient distributions, and which strikes a\nbalance between compression ratio, accuracy, and computational overhead. Our\nframework has two major novel components: sparsification of gradients in the\nfrequency domain, and a range-based floating point representation to quantize\nand further compress gradients frequencies. Both components are dynamic, with\ntunable parameters that achieve different compression ratio based on the\naccuracy requirement and systems' platforms, and achieve very high throughput\non GPUs. We prove that our techniques guarantee the convergence with a\ndiminishing compression ratio. Our experiments show that the proposed\ncompression framework effectively improves the scalability of most popular\nneural networks on a 32 GPU cluster to the baseline of no compression, without\ncompromising the accuracy and convergence speed."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Nowadays, the Internet represents a vast informational space, growing\nexponentially and the problem of search for relevant data becomes essential as\nnever before. The algorithm proposed in the article allows to perform natural\nlanguage queries on content of the document and get comprehensive meaningful\nanswers. The problem is partially solved for English as SQuAD contains enough\ndata to learn on, but there is no such dataset in Russian, so the methods used\nby scientists now are not applicable to Russian. Brain2 framework allows to\ncope with the problem - it stands out for its ability to be applied on small\ndatasets and does not require impressive computing power. The algorithm is\nillustrated on Sberbank of Russia Strategy's text and assumes the use of a\nneuromodel consisting of 65 mln synapses. The trained model is able to\nconstruct word-by-word answers to questions based on a given text. The existing\nlimitations are its current inability to identify synonyms, pronoun relations\nand allegories. Nevertheless, the results of conducted experiments showed high\ncapacity and generalisation ability of the suggested approach."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Communication overhead is the key challenge for distributed training.\nGradient compression is a widely used approach to reduce communication traffic.\nWhen combining with parallel communication mechanism method like pipeline,\ngradient compression technique can greatly alleviate the impact of\ncommunication overhead. However, there exists two problems of gradient\ncompression technique to be solved. Firstly, gradient compression brings in\nextra computation cost, which will delay the next training iteration. Secondly,\ngradient compression usually leads to the decrease of convergence accuracy."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Generative Adversarial Networks (GAN) have received wide attention in the\nmachine learning field for their potential to learn high-dimensional, complex\nreal data distribution. Specifically, they do not rely on any assumptions about\nthe distribution and can generate real-like samples from latent space in a\nsimple manner. This powerful property leads GAN to be applied to various\napplications such as image synthesis, image attribute editing, image\ntranslation, domain adaptation and other academic fields. In this paper, we aim\nto discuss the details of GAN for those readers who are familiar with, but do\nnot comprehend GAN deeply or who wish to view GAN from various perspectives. In\naddition, we explain how GAN operates and the fundamental meaning of various\nobjective functions that have been suggested recently. We then focus on how the\nGAN can be combined with an autoencoder framework. Finally, we enumerate the\nGAN variants that are applied to various tasks and other fields for those who\nare interested in exploiting GAN for their research."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Due to limited communication resources at the client and a massive number of\nmodel parameters, large-scale distributed learning tasks suffer from\ncommunication bottleneck. Gradient compression is an effective method to reduce\ncommunication load by transmitting compressed gradients. Motivated by the fact\nthat in the scenario of stochastic gradients descent, gradients between\nadjacent rounds may have a high correlation since they wish to learn the same\nmodel, this paper proposes a practical gradient compression scheme for\nfederated learning, which uses historical gradients to compress gradients and\nis based on Wyner-Ziv coding but without any probabilistic assumption. We also\nimplement our gradient quantization method on the real dataset, and the\nperformance of our method is better than the previous schemes."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Generative models are becoming increasingly popular in the literature, with\nGenerative Adversarial Networks (GAN) being the most successful variant, yet.\nWith this increasing demand and popularity, it is becoming equally difficult\nand challenging to implement and consume GAN models. A qualitative user survey\nconducted across 47 practitioners show that expert level skill is required to\nuse GAN model for a given task, despite the presence of various open source\nlibraries. In this research, we propose a novel system called AuthorGAN, aiming\nto achieve true democratization of GAN authoring. A highly modularized library\nagnostic representation of GAN model is defined to enable interoperability of\nGAN architecture across different libraries such as Keras, Tensorflow, and\nPyTorch. An intuitive drag-and-drop based visual designer is built using\nnode-red platform to enable custom architecture designing without the need for\nwriting any code. Five different GAN models are implemented as a part of this\nframework and the performance of the different GAN models are shown using the\nbenchmark MNIST dataset."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Generative Adversarial Networks (GANs) have become a dominant class of\ngenerative models. In recent years, GAN variants have yielded especially\nimpressive results in the synthesis of a variety of forms of data. Examples\ninclude compelling natural and artistic images, textures, musical sequences,\nand 3D object files. However, one obvious synthesis candidate is missing. In\nthis work, we answer one of deep learning's most pressing questions: GAN you do\nthe GAN GAN? That is, is it possible to train a GAN to model a distribution of\nGANs? We release the full source code for this project under the MIT license."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Neural networks with deep architectures have demonstrated significant\nperformance improvements in computer vision, speech recognition, and natural\nlanguage processing. The challenges in information retrieval (IR), however, are\ndifferent from these other application areas. A common form of IR involves\nranking of documents--or short passages--in response to keyword-based queries.\nEffective IR systems must deal with query-document vocabulary mismatch problem,\nby modeling relationships between different query and document terms and how\nthey indicate relevance. Models should also consider lexical matches when the\nquery contains rare terms--such as a person's name or a product model\nnumber--not seen during training, and to avoid retrieving semantically related\nbut irrelevant results. In many real-life IR tasks, the retrieval involves\nextremely large collections--such as the document index of a commercial Web\nsearch engine--containing billions of documents. Efficient IR methods should\ntake advantage of specialized IR data structures, such as inverted index, to\nefficiently retrieve from large collections. Given an information need, the IR\nsystem also mediates how much exposure an information artifact receives by\ndeciding whether it should be displayed, and where it should be positioned,\namong other results. Exposure-aware IR systems may optimize for additional\nobjectives, besides relevance, such as parity of exposure for retrieved items\nand content publishers. In this thesis, we present novel neural architectures\nand methods motivated by the specific needs and challenges of IR tasks."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Novel contexts may often arise in complex querying scenarios such as in\nevidence-based medicine (EBM) involving biomedical literature, that may not\nexplicitly refer to entities or canonical concept forms occurring in any fact-\nor rule-based knowledge source such as an ontology like the UMLS. Moreover,\nhidden associations between candidate concepts meaningful in the current\ncontext, may not exist within a single document, but within the collection, via\nalternate lexical forms. Therefore, inspired by the recent success of\nsequence-to-sequence neural models in delivering the state-of-the-art in a wide\nrange of NLP tasks, we develop a novel sequence-to-set framework with neural\nattention for learning document representations that can effect term transfer\nwithin the corpus, for semantically tagging a large collection of documents. We\ndemonstrate that our proposed method can be effective in both a supervised\nmulti-label classification setup for text categorization, as well as in a\nunique unsupervised setting with no human-annotated document labels that uses\nno external knowledge resources and only corpus-derived term statistics to\ndrive the training. Further, we show that semi-supervised training using our\narchitecture on large amounts of unlabeled data can augment performance on the\ntext categorization task when limited labeled data is available. Our approach\nto generate document encodings employing our sequence-to-set models for\ninference of semantic tags, gives to the best of our knowledge, the\nstate-of-the-art for both, the unsupervised query expansion task for the TREC\nCDS 2016 challenge dataset when evaluated on an Okapi BM25--based document\nretrieval system; and also over the MLTM baseline (Soleimani et al, 2016), for\nboth supervised and semi-supervised multi-label prediction tasks on the\ndel.icio.us and Ohsumed datasets. We will make our code and data publicly\navailable."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "This is a tutorial and survey paper on Generative Adversarial Network (GAN),\nadversarial autoencoders, and their variants. We start with explaining\nadversarial learning and the vanilla GAN. Then, we explain the conditional GAN\nand DCGAN. The mode collapse problem is introduced and various methods,\nincluding minibatch GAN, unrolled GAN, BourGAN, mixture GAN, D2GAN, and\nWasserstein GAN, are introduced for resolving this problem. Then, maximum\nlikelihood estimation in GAN are explained along with f-GAN, adversarial\nvariational Bayes, and Bayesian GAN. Then, we cover feature matching in GAN,\nInfoGAN, GRAN, LSGAN, energy-based GAN, CatGAN, MMD GAN, LapGAN, progressive\nGAN, triple GAN, LAG, GMAN, AdaGAN, CoGAN, inverse GAN, BiGAN, ALI, SAGAN,\nFew-shot GAN, SinGAN, and interpolation and evaluation of GAN. Then, we\nintroduce some applications of GAN such as image-to-image translation\n(including PatchGAN, CycleGAN, DeepFaceDrawing, simulated GAN, interactive\nGAN), text-to-image translation (including StackGAN), and mixing image\ncharacteristics (including FineGAN and MixNMatch). Finally, we explain the\nautoencoders based on adversarial learning including adversarial autoencoder,\nPixelGAN, and implicit autoencoder."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "We study distributed algorithms for expected loss minimization where the\ndatasets are large and have to be stored on different machines. Often we deal\nwith minimizing the average of a set of convex functions where each function is\nthe empirical risk of the corresponding part of the data. In the distributed\nsetting where the individual data instances can be accessed only on the local\nmachines, there would be a series of rounds of local computations followed by\nsome communication among the machines. Since the cost of the communication is\nusually higher than the local machine computations, it is important to reduce\nit as much as possible. However, we should not allow this to make the\ncomputation too expensive to become a burden in practice. Using second-order\nmethods could make the algorithms converge faster and decrease the amount of\ncommunication needed. There are some successful attempts in developing\ndistributed second-order methods. Although these methods have shown fast\nconvergence, their local computation is expensive and could enjoy more\nimprovement for practical uses. In this study we modify an existing approach,\nDANE (Distributed Approximate NEwton), in order to improve the computational\ncost while maintaining the accuracy. We tackle this problem by using iterative\nmethods for solving the local subproblems approximately instead of providing\nexact solutions for each round of communication. We study how using different\niterative methods affect the behavior of the algorithm and try to provide an\nappropriate tradeoff between the amount of local computation and the required\namount of communication. We demonstrate the practicality of our algorithm and\ncompare it to the existing distributed gradient based methods such as SGD."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Communication cost is one major bottleneck for the scalability for\ndistributed learning. One approach to reduce the communication cost is to\ncompress the gradient during communication. However, directly compressing the\ngradient decelerates the convergence speed, and the resulting algorithm may\ndiverge for biased compression. Recent work addressed this problem for\nstochastic gradient descent by adding back the compression error from the\nprevious step. This idea was further extended to one class of variance reduced\nalgorithms, where the variance of the stochastic gradient is reduced by taking\na moving average over all history gradients. However, our analysis shows that\njust adding the previous step's compression error, as done in existing work,\ndoes not fully compensate the compression error. So, we propose\nErrorCompensatedX, which uses the compression error from the previous two\nsteps. We show that ErrorCompensatedX can achieve the same asymptotic\nconvergence rate with the training without compression. Moreover, we provide a\nunified theoretical analysis framework for this class of variance reduced\nalgorithms, with or without error compensation."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "This paper studies a distributed multi-agent convex optimization problem. The\nsystem comprises multiple agents in this problem, each with a set of local data\npoints and an associated local cost function. The agents are connected to a\nserver, and there is no inter-agent communication. The agents' goal is to learn\na parameter vector that optimizes the aggregate of their local costs without\nrevealing their local data points. In principle, the agents can solve this\nproblem by collaborating with the server using the traditional distributed\ngradient-descent method. However, when the aggregate cost is ill-conditioned,\nthe gradient-descent method (i) requires a large number of iterations to\nconverge, and (ii) is highly unstable against process noise. We propose an\niterative pre-conditioning technique to mitigate the deleterious effects of the\ncost function's conditioning on the convergence rate of distributed\ngradient-descent. Unlike the conventional pre-conditioning techniques, the\npre-conditioner matrix in our proposed technique updates iteratively to\nfacilitate implementation on the distributed network. In the distributed\nsetting, we provably show that the proposed algorithm converges linearly with\nan improved rate of convergence than the traditional and adaptive\ngradient-descent methods. Additionally, for the special case when the minimizer\nof the aggregate cost is unique, our algorithm converges superlinearly. We\ndemonstrate our algorithm's superior performance compared to prominent\ndistributed algorithms for solving real logistic regression problems and\nemulating neural network training via a noisy quadratic model, thereby\nsignifying the proposed algorithm's efficiency for distributively solving\nnon-convex optimization. Moreover, we empirically show that the proposed\nalgorithm results in faster training without compromising the generalization\nperformance."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "The present paper develops a novel aggregated gradient approach for\ndistributed machine learning that adaptively compresses the gradient\ncommunication. The key idea is to first quantize the computed gradients, and\nthen skip less informative quantized gradient communications by reusing\noutdated gradients. Quantizing and skipping result in `lazy' worker-server\ncommunications, which justifies the term Lazily Aggregated Quantized gradient\nthat is henceforth abbreviated as LAQ. Our LAQ can provably attain the same\nlinear convergence rate as the gradient descent in the strongly convex case,\nwhile effecting major savings in the communication overhead both in transmitted\nbits as well as in communication rounds. Empirically, experiments with real\ndata corroborate a significant communication reduction compared to existing\ngradient- and stochastic gradient-based algorithms."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Although distributed machine learning has opened up many new and exciting\nresearch frontiers, fragmentation of models and data across different machines,\nnodes, and sites still results in considerable communication overhead, impeding\nreliable training in real-world contexts.\n  The focus on gradients as the primary shared statistic during training has\nspawned a number of intuitive algorithms for distributed deep learning;\nhowever, gradient-centric training of large deep neural networks (DNNs) tends\nto be communication-heavy, often requiring additional adaptations such as\nsparsity constraints, compression, quantization, and more, to curtail\nbandwidth.\n  We introduce an innovative, communication-friendly approach for training\ndistributed DNNs, which capitalizes on the outer-product structure of the\ngradient as revealed by the mechanics of auto-differentiation. The exposed\nstructure of the gradient evokes a new class of distributed learning algorithm,\nwhich is naturally more communication-efficient than full gradient sharing. Our\napproach, called distributed auto-differentiation (dAD), builds off a marriage\nof rank-based compression and the innate structure of the gradient as an\nouter-product. We demonstrate that dAD trains more efficiently than other state\nof the art distributed methods on modern architectures, such as transformers,\nwhen applied to large-scale text and imaging datasets. The future of\ndistributed learning, we determine, need not be dominated by gradient-centric\nalgorithms."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Federated learning (FL) is an emerging technique for training machine\nlearning models using geographically dispersed data collected by local\nentities. It includes local computation and synchronization steps. To reduce\nthe communication overhead and improve the overall efficiency of FL, gradient\nsparsification (GS) can be applied, where instead of the full gradient, only a\nsmall subset of important elements of the gradient is communicated. Existing\nwork on GS uses a fixed degree of gradient sparsity for i.i.d.-distributed data\nwithin a datacenter. In this paper, we consider adaptive degree of sparsity and\nnon-i.i.d. local datasets. We first present a fairness-aware GS method which\nensures that different clients provide a similar amount of updates. Then, with\nthe goal of minimizing the overall training time, we propose a novel online\nlearning formulation and algorithm for automatically determining the\nnear-optimal communication and computation trade-off that is controlled by the\ndegree of gradient sparsity. The online learning algorithm uses an estimated\nsign of the derivative of the objective function, which gives a regret bound\nthat is asymptotically equal to the case where exact derivative is available.\nExperiments with real datasets confirm the benefits of our proposed approaches,\nshowing up to $40\\%$ improvement in model accuracy for a finite training time."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "One of the major prerequisites for any deep learning approach is the\navailability of large-scale training data. When dealing with scanned document\nimages in real world scenarios, the principal information of its content is\nstored in the layout itself. In this work, we have proposed an automated deep\ngenerative model using Graph Neural Networks (GNNs) to generate synthetic data\nwith highly variable and plausible document layouts that can be used to train\ndocument interpretation systems, in this case, specially in digital mailroom\napplications. It is also the first graph-based approach for document layout\ngeneration task experimented on administrative document images, in this case,\ninvoices."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "This paper presents a new state-of-the-art for document image classification\nand retrieval, using features learned by deep convolutional neural networks\n(CNNs). In object and scene analysis, deep neural nets are capable of learning\na hierarchical chain of abstraction from pixel inputs to concise and\ndescriptive representations. The current work explores this capacity in the\nrealm of document analysis, and confirms that this representation strategy is\nsuperior to a variety of popular hand-crafted alternatives. Experiments also\nshow that (i) features extracted from CNNs are robust to compression, (ii) CNNs\ntrained on non-document images transfer well to document analysis tasks, and\n(iii) enforcing region-specific feature-learning is unnecessary given\nsufficient training data. This work also makes available a new labelled subset\nof the IIT-CDIP collection, containing 400,000 document images across 16\ncategories, useful for training new CNNs for document analysis."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Complex deep learning models now achieve state of the art performance for\nmany document retrieval tasks. The best models process the query or claim\njointly with the document. However for fast scalable search it is desirable to\nhave document embeddings which are independent of the claim. In this paper we\nshow that knowledge distillation can be used to encourage a model that\ngenerates claim independent document encodings to mimic the behavior of a more\ncomplex model which generates claim dependent encodings. We explore this\napproach in document retrieval for a fact extraction and verification task. We\nshow that by using the soft labels from a complex cross attention teacher\nmodel, the performance of claim independent student LSTM or CNN models is\nimproved across all the ranking metrics. The student models we use are 12x\nfaster in runtime and 20x smaller in number of parameters than the teacher"}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "In this paper, we present a distributed variant of adaptive stochastic\ngradient method for training deep neural networks in the parameter-server\nmodel. To reduce the communication cost among the workers and server, we\nincorporate two types of quantization schemes, i.e., gradient quantization and\nweight quantization, into the proposed distributed Adam. Besides, to reduce the\nbias introduced by quantization operations, we propose an error-feedback\ntechnique to compensate for the quantized gradient. Theoretically, in the\nstochastic nonconvex setting, we show that the distributed adaptive gradient\nmethod with gradient quantization and error-feedback converges to the\nfirst-order stationary point, and that the distributed adaptive gradient method\nwith weight quantization and error-feedback converges to the point related to\nthe quantized level under both the single-worker and multi-worker modes. At\nlast, we apply the proposed distributed adaptive gradient methods to train deep\nneural networks. Experimental results demonstrate the efficacy of our methods."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "We consider a request processing system composed of organizations and their\nservers connected by the Internet.\n  The latency a user observes is a sum of communication delays and the time\nneeded to handle the request on a server. The handling time depends on the\nserver congestion, i.e. the total number of requests a server must handle. We\nanalyze the problem of balancing the load in a network of servers in order to\nminimize the total observed latency. We consider both cooperative and selfish\norganizations (each organization aiming to minimize the latency of the\nlocally-produced requests). The problem can be generalized to the task\nscheduling in a distributed cloud; or to content delivery in an\norganizationally-distributed CDNs.\n  In a cooperative network, we show that the problem is polynomially solvable.\nWe also present a distributed algorithm iteratively balancing the load. We show\nhow to estimate the distance between the current solution and the optimum based\non the amount of load exchanged by the algorithm. During the experimental\nevaluation, we show that the distributed algorithm is efficient, therefore it\ncan be used in networks with dynamically changing loads.\n  In a network of selfish organizations, we prove that the price of anarchy\n(the worst-case loss of performance due to selfishness) is low when the network\nis homogeneous and the servers are loaded (the request handling time is high\ncompared to the communication delay). After relaxing these assumptions, we\nassess the loss of performance caused by the selfishness experimentally,\nshowing that it remains low.\n  Our results indicate that a network of servers handling requests can be\nefficiently managed by a distributed algorithm. Additionally, even if the\nnetwork is organizationally distributed, with individual organizations\noptimizing performance of their requests, the network remains efficient."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "We consider large scale distributed optimization over a set of edge devices\nconnected to a central server, where the limited communication bandwidth\nbetween the server and edge devices imposes a significant bottleneck for the\noptimization procedure. Inspired by recent advances in federated learning, we\npropose a distributed stochastic gradient descent (SGD) type algorithm that\nexploits the sparsity of the gradient, when possible, to reduce communication\nburden. At the heart of the algorithm is to use compressed sensing techniques\nfor the compression of the local stochastic gradients at the device side; and\nat the server side, a sparse approximation of the global stochastic gradient is\nrecovered from the noisy aggregated compressed local gradients. We conduct\ntheoretical analysis on the convergence of our algorithm in the presence of\nnoise perturbation incurred by the communication channels, and also conduct\nnumerical experiments to corroborate its effectiveness."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Due to the increasing amount of data on the internet, finding a\nhighly-informative, low-dimensional representation for text is one of the main\nchallenges for efficient natural language processing tasks including text\nclassification. This representation should capture the semantic information of\nthe text while retaining their relevance level for document classification.\nThis approach maps the documents with similar topics to a similar space in\nvector space representation. To obtain representation for large text, we\npropose the utilization of deep Siamese neural networks. To embed document\nrelevance in topics in the distributed representation, we use a Siamese neural\nnetwork to jointly learn document representations. Our Siamese network consists\nof two sub-network of multi-layer perceptron. We examine our representation for\nthe text categorization task on BBC news dataset. The results show that the\nproposed representations outperform the conventional and state-of-the-art\nrepresentations in the text classification task on this dataset."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "How to leverage cross-document interactions to improve ranking performance is\nan important topic in information retrieval (IR) research. However, this topic\nhas not been well-studied in the learning-to-rank setting and most of the\nexisting work still treats each document independently while scoring. The\nrecent development of deep learning shows strength in modeling complex\nrelationships across sequences and sets. It thus motivates us to study how to\nleverage cross-document interactions for learning-to-rank in the deep learning\nframework. In this paper, we formally define the permutation-equivariance\nrequirement for a scoring function that captures cross-document interactions.\nWe then propose a self-attention based document interaction network and show\nthat it satisfies the permutation-equivariant requirement, and can generate\nscores for document sets of varying sizes. Our proposed methods can\nautomatically learn to capture document interactions without any auxiliary\ninformation, and can scale across large document sets. We conduct experiments\non three ranking datasets: the benchmark Web30k, a Gmail search, and a Google\nDrive Quick Access dataset. Experimental results show that our proposed methods\nare both more effective and efficient than baselines."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Graph Neural Networks (GNNs) have received significant attention due to their\nstate-of-the-art performance on various graph representation learning tasks.\nHowever, recent studies reveal that GNNs are vulnerable to adversarial attacks,\ni.e. an attacker is able to fool the GNNs by perturbing the graph structure or\nnode features deliberately. While being able to successfully decrease the\nperformance of GNNs, most existing attacking algorithms require access to\neither the model parameters or the training data, which is not practical in the\nreal world.\n  In this paper, we develop deeper insights into the Mettack algorithm, which\nis a representative grey-box attacking method, and then we propose a\ngradient-based black-box attacking algorithm. Firstly, we show that the Mettack\nalgorithm will perturb the edges unevenly, thus the attack will be highly\ndependent on a specific training set. As a result, a simple yet useful strategy\nto defense against Mettack is to train the GNN with the validation set.\nSecondly, to overcome the drawbacks, we propose the Black-Box Gradient Attack\n(BBGA) algorithm. Extensive experiments demonstrate that out proposed method is\nable to achieve stable attack performance without accessing the training sets\nof the GNNs. Further results shows that our proposed method is also applicable\nwhen attacking against various defense methods."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "In federated learning, communication cost is often a critical bottleneck to\nscale up distributed optimization algorithms to collaboratively learn a model\nfrom millions of devices with potentially unreliable or limited communication\nand heterogeneous data distributions. Two notable trends to deal with the\ncommunication overhead of federated algorithms are gradient compression and\nlocal computation with periodic communication. Despite many attempts,\ncharacterizing the relationship between these two approaches has proven\nelusive. We address this by proposing a set of algorithms with periodical\ncompressed (quantized or sparsified) communication and analyze their\nconvergence properties in both homogeneous and heterogeneous local data\ndistribution settings. For the homogeneous setting, our analysis improves\nexisting bounds by providing tighter convergence rates for both strongly convex\nand non-convex objective functions. To mitigate data heterogeneity, we\nintroduce a local gradient tracking scheme and obtain sharp convergence rates\nthat match the best-known communication complexities without compression for\nconvex, strongly convex, and nonconvex settings. We complement our theoretical\nresults and demonstrate the effectiveness of our proposed methods by several\nexperiments on real-world datasets."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Recent approaches in literature have exploited the multi-modal information in\ndocuments (text, layout, image) to serve specific downstream document tasks.\nHowever, they are limited by their - (i) inability to learn cross-modal\nrepresentations across text, layout and image dimensions for documents and (ii)\ninability to process multi-page documents. Pre-training techniques have been\nshown in Natural Language Processing (NLP) domain to learn generic textual\nrepresentations from large unlabelled datasets, applicable to various\ndownstream NLP tasks. In this paper, we propose a multi-task learning-based\nframework that utilizes a combination of self-supervised and supervised\npre-training tasks to learn a generic document representation applicable to\nvarious downstream document tasks. Specifically, we introduce Document Topic\nModelling and Document Shuffle Prediction as novel pre-training tasks to learn\nrich image representations along with the text and layout representations for\ndocuments. We utilize the Longformer network architecture as the backbone to\nencode the multi-modal information from multi-page documents in an end-to-end\nfashion. We showcase the applicability of our pre-training framework on a\nvariety of different real-world document tasks such as document classification,\ndocument information extraction, and document retrieval. We evaluate our\nframework on different standard document datasets and conduct exhaustive\nexperiments to compare performance against various ablations of our framework\nand state-of-the-art baselines."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Gradient quantization is an emerging technique in reducing communication\ncosts in distributed learning. Existing gradient quantization algorithms often\nrely on engineering heuristics or empirical observations, lacking a systematic\napproach to dynamically quantize gradients. This paper addresses this issue by\nproposing a novel dynamically quantized SGD (DQ-SGD) framework, enabling us to\ndynamically adjust the quantization scheme for each gradient descent step by\nexploring the trade-off between communication cost and convergence error. We\nderive an upper bound, tight in some cases, of the convergence error for a\nrestricted family of quantization schemes and loss functions. We design our\nDQ-SGD algorithm via minimizing the communication cost under the convergence\nerror constraints. Finally, through extensive experiments on large-scale\nnatural language processing and computer vision tasks on AG-News, CIFAR-10, and\nCIFAR-100 datasets, we demonstrate that our quantization scheme achieves better\ntradeoffs between the communication cost and learning performance than other\nstate-of-the-art gradient quantization methods."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "GANS are powerful generative models that are able to model the manifold of\nnatural images. We leverage this property to perform manifold regularization by\napproximating the Laplacian norm using a Monte Carlo approximation that is\neasily computed with the GAN. When incorporated into the feature-matching GAN\nof Improved GAN, we achieve state-of-the-art results for GAN-based\nsemi-supervised learning on the CIFAR-10 dataset, with a method that is\nsignificantly easier to implement than competing methods."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Deep Neural Networks have gained significant attraction due to their wide\napplicability in different domains. DNN sizes and training samples are\nconstantly growing, making training of such workloads more challenging.\nDistributed training is a solution to reduce the training time.\nHigh-performance distributed training platforms should leverage\nmulti-dimensional hierarchical networks, which interconnect accelerators\nthrough different levels of the network, to dramatically reduce expensive NICs\nrequired for the scale-out network. However, it comes at the expense of\ncommunication overhead between distributed accelerators to exchange gradients\nor input/output activation. In order to allow for further scaling of the\nworkloads, communication overhead needs to be minimized. In this paper, we\nmotivate the fact that in training platforms, adding more intermediate network\ndimensions is beneficial for efficiently mitigating the excessive use of\nexpensive NIC resources. Further, we address different challenges of the DNN\ntraining on hierarchical networks. We discuss when designing the interconnect,\nhow to distribute network bandwidth resources across different dimensions in\norder to (i) maximize BW utilization of all dimensions, and (ii) minimizing the\noverall training time for the target workload. We then implement a framework\nthat, for a given workload, determines the best network configuration that\nmaximizes performance, or performance-per-cost."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Modern large scale machine learning applications require stochastic\noptimization algorithms to be implemented on distributed computational\narchitectures. A key bottleneck is the communication overhead for exchanging\ninformation such as stochastic gradients among different workers. In this\npaper, to reduce the communication cost we propose a convex optimization\nformulation to minimize the coding length of stochastic gradients. To solve the\noptimal sparsification efficiently, several simple and fast algorithms are\nproposed for approximate solution, with theoretical guaranteed for sparseness.\nExperiments on $\\ell_2$ regularized logistic regression, support vector\nmachines, and convolutional neural networks validate our sparsification\napproaches."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Compressed communication, in the form of sparsification or quantization of\nstochastic gradients, is employed to reduce communication costs in distributed\ndata-parallel training of deep neural networks. However, there exists a\ndiscrepancy between theory and practice: while theoretical analysis of most\nexisting compression methods assumes compression is applied to the gradients of\nthe entire model, many practical implementations operate individually on the\ngradients of each layer of the model. In this paper, we prove that layer-wise\ncompression is, in theory, better, because the convergence rate is upper\nbounded by that of entire-model compression for a wide range of biased and\nunbiased compression methods. However, despite the theoretical bound, our\nexperimental study of six well-known methods shows that convergence, in\npractice, may or may not be better, depending on the actual trained model and\ncompression ratio. Our findings suggest that it would be advantageous for deep\nlearning frameworks to include support for both layer-wise and entire-model\ncompression."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "One major drawback of state of the art Neural Networks (NN)-based approaches\nfor document classification purposes is the large number of training samples\nrequired to obtain an efficient classification. The minimum required number is\naround one thousand annotated documents for each class. In many cases it is\nvery difficult, if not impossible, to gather this number of samples in real\nindustrial processes. In this paper, we analyse the efficiency of NN-based\ndocument classification systems in a sub-optimal training case, based on the\nsituation of a company document stream. We evaluated three different\napproaches, one based on image content and two on textual content. The\nevaluation was divided into four parts: a reference case, to assess the\nperformance of the system in the lab; two cases that each simulate a specific\ndifficulty linked to document stream processing; and a realistic case that\ncombined all of these difficulties. The realistic case highlighted the fact\nthat there is a significant drop in the efficiency of NN-Based document\nclassification systems. Although they remain efficient for well represented\nclasses (with an over-fitting of the system for those classes), it is\nimpossible for them to handle appropriately less well represented classes.\nNN-Based document classification systems need to be adapted to resolve these\ntwo problems before they can be considered for use in a company document\nstream."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Deep neural networks (DNNs) have been extremely successful in solving many\nchallenging AI tasks in natural language processing, speech recognition, and\ncomputer vision nowadays. However, DNNs are typically computation intensive,\nmemory demanding, and power hungry, which significantly limits their usage on\nplatforms with constrained resources. Therefore, a variety of compression\ntechniques (e.g. quantization, pruning, and knowledge distillation) have been\nproposed to reduce the size and power consumption of DNNs. Blockwise knowledge\ndistillation is one of the compression techniques that can effectively reduce\nthe size of a highly complex DNN. However, it is not widely adopted due to its\nlong training time. In this paper, we propose a novel parallel blockwise\ndistillation algorithm to accelerate the distillation process of sophisticated\nDNNs. Our algorithm leverages local information to conduct independent\nblockwise distillation, utilizes depthwise separable layers as the efficient\nreplacement block architecture, and properly addresses limiting factors (e.g.\ndependency, synchronization, and load balancing) that affect parallelism. The\nexperimental results running on an AMD server with four Geforce RTX 2080Ti GPUs\nshow that our algorithm can achieve 3x speedup plus 19% energy savings on VGG\ndistillation, and 3.5x speedup plus 29% energy savings on ResNet distillation,\nboth with negligible accuracy loss. The speedup of ResNet distillation can be\nfurther improved to 3.87 when using four RTX6000 GPUs in a distributed cluster."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Sign-based algorithms (e.g. signSGD) have been proposed as a biased gradient\ncompression technique to alleviate the communication bottleneck in training\nlarge neural networks across multiple workers. We show simple convex\ncounter-examples where signSGD does not converge to the optimum. Further, even\nwhen it does converge, signSGD may generalize poorly when compared with SGD.\nThese issues arise because of the biased nature of the sign compression\noperator. We then show that using error-feedback, i.e. incorporating the error\nmade by the compression operator into the next step, overcomes these issues. We\nprove that our algorithm EF-SGD with arbitrary compression operator achieves\nthe same rate of convergence as SGD without any additional assumptions. Thus\nEF-SGD achieves gradient compression for free. Our experiments thoroughly\nsubstantiate the theory and show that error-feedback improves both convergence\nand generalization. Code can be found at\n\\url{https://github.com/epfml/error-feedback-SGD}."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "In both the fields of computer science and medicine there is very strong\ninterest in developing personalized treatment policies for patients who have\nvariable responses to treatments. In particular, I aim to find an optimal\npersonalized treatment policy which is a non-deterministic function of the\npatient specific covariate data that maximizes the expected survival time or\nclinical outcome. I developed an algorithmic framework to solve multistage\ndecision problem with a varying number of stages that are subject to censoring\nin which the \"rewards\" are expected survival times. In specific, I developed a\nnovel Q-learning algorithm that dynamically adjusts for these parameters.\nFurthermore, I found finite upper bounds on the generalized error of the\ntreatment paths constructed by this algorithm. I have also shown that when the\noptimal Q-function is an element of the approximation space, the anticipated\nsurvival times for the treatment regime constructed by the algorithm will\nconverge to the optimal treatment path. I demonstrated the performance of the\nproposed algorithmic framework via simulation studies and through the analysis\nof chronic depression data and a hypothetical clinical trial. The censored\nQ-learning algorithm I developed is more effective than the state of the art\nclinical decision support systems and is able to operate in environments when\nmany covariate parameters may be unobtainable or censored."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "In information retrieval (IR) and related tasks, term weighting approaches\ntypically consider the frequency of the term in the document and in the\ncollection in order to compute a score reflecting the importance of the term\nfor the document. In tasks characterized by the presence of training data (such\nas text classification) it seems logical that the term weighting function\nshould take into account the distribution (as estimated from training data) of\nthe term across the classes of interest. Although `supervised term weighting'\napproaches that use this intuition have been described before, they have failed\nto show consistent improvements. In this article we analyse the possible\nreasons for this failure, and call consolidated assumptions into question.\nFollowing this criticism we propose a novel supervised term weighting approach\nthat, instead of relying on any predefined formula, learns a term weighting\nfunction optimised on the training set of interest; we dub this approach\n\\emph{Learning to Weight} (LTW). The experiments that we run on several\nwell-known benchmarks, and using different learning methods, show that our\nmethod outperforms previous term weighting approaches in text classification."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "This article is in the context of gradient compression. Gradient compression\nis a popular technique for mitigating the communication bottleneck observed\nwhen training large machine learning models in a distributed manner using\ngradient-based methods such as stochastic gradient descent. In this article,\nassuming a Gaussian distribution for the components in gradient, we find the\nrate distortion trade-off of gradient quantization schemes such as Scaled-sign\nand Top-K, and compare with the Shannon rate distortion limit. A similar\ncomparison with vector quantizers also is presented."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Adam is shown not being able to converge to the optimal solution in certain\ncases. Researchers recently propose several algorithms to avoid the issue of\nnon-convergence of Adam, but their efficiency turns out to be unsatisfactory in\npractice. In this paper, we provide new insight into the non-convergence issue\nof Adam as well as other adaptive learning rate methods. We argue that there\nexists an inappropriate correlation between gradient $g_t$ and the\nsecond-moment term $v_t$ in Adam ($t$ is the timestep), which results in that a\nlarge gradient is likely to have small step size while a small gradient may\nhave a large step size. We demonstrate that such biased step sizes are the\nfundamental cause of non-convergence of Adam, and we further prove that\ndecorrelating $v_t$ and $g_t$ will lead to unbiased step size for each\ngradient, thus solving the non-convergence problem of Adam. Finally, we propose\nAdaShift, a novel adaptive learning rate method that decorrelates $v_t$ and\n$g_t$ by temporal shifting, i.e., using temporally shifted gradient $g_{t-n}$\nto calculate $v_t$. The experiment results demonstrate that AdaShift is able to\naddress the non-convergence issue of Adam, while still maintaining a\ncompetitive performance with Adam in terms of both training speed and\ngeneralization."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Humans can naturally learn to execute a new task by seeing it performed by\nother individuals once, and then reproduce it in a variety of configurations.\nEndowing robots with this ability of imitating humans from third person is a\nvery immediate and natural way of teaching new tasks. Only recently, through\nmeta-learning, there have been successful attempts to one-shot imitation\nlearning from humans; however, these approaches require a lot of human\nresources to collect the data in the real world to train the robot. But is\nthere a way to remove the need for real world human demonstrations during\ntraining? We show that with Task-Embedded Control Networks, we can infer\ncontrol polices by embedding human demonstrations that can condition a control\npolicy and achieve one-shot imitation learning. Importantly, we do not use a\nreal human arm to supply demonstrations during training, but instead leverage\ndomain randomisation in an application that has not been seen before:\nsim-to-real transfer on humans. Upon evaluating our approach on pushing and\nplacing tasks in both simulation and in the real world, we show that in\ncomparison to a system that was trained on real-world data we are able to\nachieve similar results by utilising only simulation data."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Rapid increase of digitized document give birth to high demand of document\nimage retrieval. While conventional document image retrieval approaches depend\non complex OCR-based text recognition and text similarity detection, this paper\nproposes a new content-based approach, in which more attention is paid to\nfeatures extraction and fusion. In the proposed approach, multiple features of\ndocument images are extracted by different CNN models. After that, the\nextracted CNN features are reduced and fused into weighted average feature.\nFinally, the document images are ranked based on feature similarity to a\nprovided query image. Experimental procedure is performed on a group of\ndocument images that transformed from academic papers, which contain both\nEnglish and Chinese document, the results show that the proposed approach has\ngood ability to retrieve document images with similar text content, and the\nfusion of CNN features can effectively improve the retrieval accuracy."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Extracting information from full documents is an important problem in many\ndomains, but most previous work focus on identifying relationships within a\nsentence or a paragraph. It is challenging to create a large-scale information\nextraction (IE) dataset at the document level since it requires an\nunderstanding of the whole document to annotate entities and their\ndocument-level relationships that usually span beyond sentences or even\nsections. In this paper, we introduce SciREX, a document level IE dataset that\nencompasses multiple IE tasks, including salient entity identification and\ndocument level $N$-ary relation identification from scientific articles. We\nannotate our dataset by integrating automatic and human annotations, leveraging\nexisting scientific knowledge resources. We develop a neural model as a strong\nbaseline that extends previous state-of-the-art IE models to document-level IE.\nAnalyzing the model performance shows a significant gap between human\nperformance and current baselines, inviting the community to use our dataset as\na challenge to develop document-level IE models. Our data and code are publicly\navailable at https://github.com/allenai/SciREX"}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "BERT based ranking models have achieved superior performance on various\ninformation retrieval tasks. However, the large number of parameters and\ncomplex self-attention operation come at a significant latency overhead. To\nremedy this, recent works propose late-interaction architectures, which allow\npre-computation of intermediate document representations, thus reducing the\nruntime latency. Nonetheless, having solved the immediate latency issue, these\nmethods now introduce storage costs and network fetching latency, which limits\ntheir adoption in real-life production systems.\n  In this work, we propose the Succinct Document Representation (SDR) scheme\nthat computes highly compressed intermediate document representations,\nmitigating the storage/network issue. Our approach first reduces the dimension\nof token representations by encoding them using a novel autoencoder\narchitecture that uses the document's textual content in both the encoding and\ndecoding phases. After this token encoding step, we further reduce the size of\nentire document representations using a modern quantization technique.\n  Extensive evaluations on passage re-reranking on the MSMARCO dataset show\nthat compared to existing approaches using compressed document representations,\nour method is highly efficient, achieving 4x-11.6x better compression rates for\nthe same ranking quality."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "In this paper, we proposed a new technique, {\\em variance controlled\nstochastic gradient} (VCSG), to improve the performance of the stochastic\nvariance reduced gradient (SVRG) algorithm. To avoid over-reducing the variance\nof gradient by SVRG, a hyper-parameter $\\lambda$ is introduced in VCSG that is\nable to control the reduced variance of SVRG. Theory shows that the\noptimization method can converge by using an unbiased gradient estimator, but\nin practice, biased gradient estimation can allow more efficient convergence to\nthe vicinity since an unbiased approach is computationally more expensive.\n$\\lambda$ also has the effect of balancing the trade-off between unbiased and\nbiased estimations. Secondly, to minimize the number of full gradient\ncalculations in SVRG, a variance-bounded batch is introduced to reduce the\nnumber of gradient calculations required in each iteration. For smooth\nnon-convex functions, the proposed algorithm converges to an approximate\nfirst-order stationary point (i.e.\n$\\mathbb{E}\\|\\nabla{f}(x)\\|^{2}\\leq\\epsilon$) within\n$\\mathcal{O}(min\\{1/\\epsilon^{3/2},n^{1/4}/\\epsilon\\})$ number of stochastic\ngradient evaluations, which improves the leading gradient complexity of\nstochastic gradient-based method SCS\n$(\\mathcal{O}(min\\{1/\\epsilon^{5/3},n^{2/3}/\\epsilon\\})$. It is shown\ntheoretically and experimentally that VCSG can be deployed to improve\nconvergence."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Machine learning systems based on deep neural networks (DNNs) produce\nstate-of-the-art results in many applications. Considering the large amount of\ntraining data and know-how required to generate the network, it is more\npractical to use third-party DNN intellectual property (IP) cores for many\ndesigns. No doubt to say, it is essential for DNN IP vendors to provide test\ncases for functional validation without leaking their parameters to IP users.\nTo satisfy this requirement, we propose to effectively generate test cases that\nactivate parameters as many as possible and propagate their perturbations to\noutputs. Then the functionality of DNN IPs can be validated by only checking\ntheir outputs. However, it is difficult considering large numbers of parameters\nand highly non-linearity of DNNs. In this paper, we tackle this problem by\njudiciously selecting samples from the DNN training set and applying a\ngradient-based method to generate new test cases. Experimental results\ndemonstrate the efficacy of our proposed solution."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Data sets are growing in complexity thanks to the increasing facilities we\nhave nowadays to both generate and store data. This poses many challenges to\nmachine learning that are leading to the proposal of new methods and paradigms,\nin order to be able to deal with what is nowadays referred to as Big Data. In\nthis paper we propose a method for the aggregation of different Bayesian\nnetwork structures that have been learned from separate data sets, as a first\nstep towards mining data sets that need to be partitioned in an horizontal way,\ni.e. with respect to the instances, in order to be processed. Considerations\nthat should be taken into account when dealing with this situation are\ndiscussed. Scalable learning of Bayesian networks is slowly emerging, and our\nmethod constitutes one of the first insights into Gaussian Bayesian network\naggregation from different sources. Tested on synthetic data it obtains good\nresults that surpass those from individual learning. Future research will be\nfocused on expanding the method and testing more diverse data sets."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Modern entity linking systems rely on large collections of documents\nspecifically annotated for the task (e.g., AIDA CoNLL). In contrast, we propose\nan approach which exploits only naturally occurring information: unlabeled\ndocuments and Wikipedia. Our approach consists of two stages. First, we\nconstruct a high recall list of candidate entities for each mention in an\nunlabeled document. Second, we use the candidate lists as weak supervision to\nconstrain our document-level entity linking model. The model treats entities as\nlatent variables and, when estimated on a collection of unlabelled texts,\nlearns to choose entities relying both on local context of each mention and on\ncoherence with other entities in the document. The resulting approach rivals\nfully-supervised state-of-the-art systems on standard test sets. It also\napproaches their performance in the very challenging setting: when tested on a\ntest set sampled from the data used to estimate the supervised systems. By\ncomparing to Wikipedia-only training of our model, we demonstrate that modeling\nunlabeled documents is beneficial."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Most of the textual information available to us are temporally variable. In a\nworld where information is dynamic, time-stamping them is a very important\ntask. Documents are a good source of information and are used for many tasks\nlike, sentiment analysis, classification of reviews etc. The knowledge of\ncreation date of documents facilitates several tasks like summarization, event\nextraction, temporally focused information extraction etc. Unfortunately, for\nmost of the documents on the web, the time-stamp meta-data is either erroneous\nor missing. Thus document dating is a challenging problem which requires\ninference over the temporal structure of the document alongside the contextual\ninformation of the document. Prior document dating systems have largely relied\non handcrafted features while ignoring such document-internal structures. In\nthis paper we propose NeuralDater, a Graph Convolutional Network (GCN) based\ndocument dating approach which jointly exploits syntactic and temporal graph\nstructures of document in a principled way. We also pointed out some\nlimitations of NeuralDater and tried to utilize both context and temporal\ninformation in documents in a more flexible and intuitive manner proposing AD3:\nAttentive Deep Document Dater, an attention-based document dating system. To\nthe best of our knowledge these are the first application of deep learning\nmethods for the task. Through extensive experiments on real-world datasets, we\nfind that our models significantly outperforms state-of-the-art baselines by a\nsignificant margin."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Often, when dealing with real-world recognition problems, we do not need, and\noften cannot have, knowledge of the entire set of possible classes that might\nappear during operational testing. In such cases, we need to think of robust\nclassification methods able to deal with the \"unknown\" and properly reject\nsamples belonging to classes never seen during training. Notwithstanding,\nexisting classifiers to date were mostly developed for the closed-set scenario,\ni.e., the classification setup in which it is assumed that all test samples\nbelong to one of the classes with which the classifier was trained. In the\nopen-set scenario, however, a test sample can belong to none of the known\nclasses and the classifier must properly reject it by classifying it as\nunknown. In this work, we extend upon the well-known Support Vector Machines\n(SVM) classifier and introduce the Open-Set Support Vector Machines (OSSVM),\nwhich is suitable for recognition in open-set setups. OSSVM balances the\nempirical risk and the risk of the unknown and ensures that the region of the\nfeature space in which a test sample would be classified as known (one of the\nknown classes) is always bounded, ensuring a finite risk of the unknown. In\nthis work, we also highlight the properties of the SVM classifier related to\nthe open-set scenario, and provide necessary and sufficient conditions for an\nRBF SVM to have bounded open-space risk."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Traditional Relational Topic Models provide a way to discover the hidden\ntopics from a document network. Many theoretical and practical tasks, such as\ndimensional reduction, document clustering, link prediction, benefit from this\nrevealed knowledge. However, existing relational topic models are based on an\nassumption that the number of hidden topics is known in advance, and this is\nimpractical in many real-world applications. Therefore, in order to relax this\nassumption, we propose a nonparametric relational topic model in this paper.\nInstead of using fixed-dimensional probability distributions in its generative\nmodel, we use stochastic processes. Specifically, a gamma process is assigned\nto each document, which represents the topic interest of this document.\nAlthough this method provides an elegant solution, it brings additional\nchallenges when mathematically modeling the inherent network structure of\ntypical document network, i.e., two spatially closer documents tend to have\nmore similar topics. Furthermore, we require that the topics are shared by all\nthe documents. In order to resolve these challenges, we use a subsampling\nstrategy to assign each document a different gamma process from the global\ngamma process, and the subsampling probabilities of documents are assigned with\na Markov Random Field constraint that inherits the document network structure.\nThrough the designed posterior inference algorithm, we can discover the hidden\ntopics and its number simultaneously. Experimental results on both synthetic\nand real-world network datasets demonstrate the capabilities of learning the\nhidden topics and, more importantly, the number of topics."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "In this work, a region-based Deep Convolutional Neural Network framework is\nproposed for document structure learning. The contribution of this work\ninvolves efficient training of region based classifiers and effective\nensembling for document image classification. A primary level of `inter-domain'\ntransfer learning is used by exporting weights from a pre-trained VGG16\narchitecture on the ImageNet dataset to train a document classifier on whole\ndocument images. Exploiting the nature of region based influence modelling, a\nsecondary level of `intra-domain' transfer learning is used for rapid training\nof deep learning models for image segments. Finally, stacked generalization\nbased ensembling is utilized for combining the predictions of the base deep\nneural network models. The proposed method achieves state-of-the-art accuracy\nof 92.2% on the popular RVL-CDIP document image dataset, exceeding benchmarks\nset by existing algorithms."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Latent topic models have been successfully applied as an unsupervised topic\ndiscovery technique in large document collections. With the proliferation of\nhypertext document collection such as the Internet, there has also been great\ninterest in extending these approaches to hypertext [6, 9]. These approaches\ntypically model links in an analogous fashion to how they model words - the\ndocument-link co-occurrence matrix is modeled in the same way that the\ndocument-word co-occurrence matrix is modeled in standard topic models. In this\npaper we present a probabilistic generative model for hypertext document\ncollections that explicitly models the generation of links. Specifically, links\nfrom a word w to a document d depend directly on how frequent the topic of w is\nin d, in addition to the in-degree of d. We show how to perform EM learning on\nthis model efficiently. By not modeling links as analogous to words, we end up\nusing far fewer free parameters and obtain better link prediction results."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "The performance of neural decoders can degrade over time due to\nnonstationarities in the relationship between neuronal activity and behavior.\nIn this case, brain-machine interfaces (BMI) require adaptation of their\ndecoders to maintain high performance across time. One way to achieve this is\nby use of periodical calibration phases, during which the BMI system (or an\nexternal human demonstrator) instructs the user to perform certain movements or\nbehaviors. This approach has two disadvantages: (i) calibration phases\ninterrupt the autonomous operation of the BMI and (ii) between two calibration\nphases the BMI performance might not be stable but continuously decrease. A\nbetter alternative would be that the BMI decoder is able to continuously adapt\nin an unsupervised manner during autonomous BMI operation, i.e. without knowing\nthe movement intentions of the user.\n  In the present article, we present an efficient method for such unsupervised\ntraining of BMI systems for continuous movement control. The proposed method\nutilizes a cost function derived from neuronal recordings, which guides a\nlearning algorithm to evaluate the decoding parameters. We verify the\nperformance of our adaptive method by simulating a BMI user with an optimal\nfeedback control model and its interaction with our adaptive BMI decoder. The\nsimulation results show that the cost function and the algorithm yield fast and\nprecise trajectories towards targets at random orientations on a 2-dimensional\ncomputer screen. For initially unknown and non-stationary tuning parameters,\nour unsupervised method is still able to generate precise trajectories and to\nkeep its performance stable in the long term. The algorithm can optionally work\nalso with neuronal error signals instead or in conjunction with the proposed\nunsupervised adaptation."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "We present a novel Bayesian topic model for learning discourse-level document\nstructure. Our model leverages insights from discourse theory to constrain\nlatent topic assignments in a way that reflects the underlying organization of\ndocument topics. We propose a global model in which both topic selection and\nordering are biased to be similar across a collection of related documents. We\nshow that this space of orderings can be effectively represented using a\ndistribution over permutations called the Generalized Mallows Model. We apply\nour method to three complementary discourse-level tasks: cross-document\nalignment, document segmentation, and information ordering. Our experiments\nshow that incorporating our permutation-based model in these applications\nyields substantial improvements in performance over previously proposed\nmethods."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "We propose a Label Propagation based algorithm for weakly supervised text\nclassification. We construct a graph where each document is represented by a\nnode and edge weights represent similarities among the documents. Additionally,\nwe discover underlying topics using Latent Dirichlet Allocation (LDA) and\nenrich the document graph by including the topics in the form of additional\nnodes. The edge weights between a topic and a text document represent level of\n\"affinity\" between them. Our approach does not require document level\nlabelling, instead it expects manual labels only for topic nodes. This\nsignificantly minimizes the level of supervision needed as only a few topics\nare observed to be enough for achieving sufficiently high accuracy. The Label\nPropagation Algorithm is employed on this enriched graph to propagate labels\namong the nodes. Our approach combines the advantages of Label Propagation\n(through document-document similarities) and Topic Modelling (for minimal but\nsmart supervision). We demonstrate the effectiveness of our approach on various\ndatasets and compare with state-of-the-art weakly supervised text\nclassification approaches."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "We introduce a novel type of text representation that preserves the 2D layout\nof a document. This is achieved by encoding each document page as a\ntwo-dimensional grid of characters. Based on this representation, we present a\ngeneric document understanding pipeline for structured documents. This pipeline\nmakes use of a fully convolutional encoder-decoder network that predicts a\nsegmentation mask and bounding boxes. We demonstrate its capabilities on an\ninformation extraction task from invoices and show that it significantly\noutperforms approaches based on sequential text or document images."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Learning predictive models from interaction with the world allows an agent,\nsuch as a robot, to learn about how the world works, and then use this learned\nmodel to plan coordinated sequences of actions to bring about desired outcomes.\nHowever, learning a model that captures the dynamics of complex skills\nrepresents a major challenge: if the agent needs a good model to perform these\nskills, it might never be able to collect the experience on its own that is\nrequired to learn these delicate and complex behaviors. Instead, we can imagine\naugmenting the training set with observational data of other agents, such as\nhumans. Such data is likely more plentiful, but represents a different\nembodiment. For example, videos of humans might show a robot how to use a tool,\nbut (i) are not annotated with suitable robot actions, and (ii) contain a\nsystematic distributional shift due to the embodiment differences between\nhumans and robots. We address the first challenge by formulating the\ncorresponding graphical model and treating the action as an observed variable\nfor the interaction data and an unobserved variable for the observation data,\nand the second challenge by using a domain-dependent prior. In addition to\ninteraction data, our method is able to leverage videos of passive observations\nin a driving dataset and a dataset of robotic manipulation videos. A robotic\nplanning agent equipped with our method can learn to use tools in a tabletop\nrobotic manipulation setting by observing humans without ever seeing a robotic\nvideo of tool use."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "Document categorization is a technique where the category of a document is\ndetermined. In this paper three well-known supervised learning techniques which\nare Support Vector Machine(SVM), Na\\\"ive Bayes(NB) and Stochastic Gradient\nDescent(SGD) compared for Bengali document categorization. Besides classifier,\nclassification also depends on how feature is selected from dataset. For\nanalyzing those classifier performances on predicting a document against twelve\ncategories several feature selection techniques are also applied in this\narticle namely Chi square distribution, normalized TFIDF (term\nfrequency-inverse document frequency) with word analyzer. So, we attempt to\nexplore the efficiency of those three-classification algorithms by using two\ndifferent feature selection techniques in this article."}
{"query": "The query consists of users’ needs, leading to several research questions that span a paragraph. Each candidate passage is an abstract from a scientific paper. The objective of this information retrieval task is to identify the abstract that most effectively meets the user's needs in the query. [SEP] I need to train a GAN that can generate high-quality outputs for various applications. To do this, I need a system that is able to store and manage the massive amount of training data. Since I have sufficient computing resources, my plan is to use distributed training methods to handle the huge amount of data. In order to minimize the communication cost in the distributed system, I think an algorithm using quantized gradient might work. Additionally, I also want the algorithm to be able to avoid convergence issues, which I believe can be handled by the gradient compression method. With such an algorithm, I can have a GAN that is able to be trained on various dataset including wikipedia, github and other sources from the internet, and try to develop a model that is able to have conversations with human naturally.", "corpus": "With the increase of information, document classification as one of the\nmethods of text mining, plays vital role in many management and organizing\ninformation. Document classification is the process of assigning a document to\none or more predefined category labels. Document classification includes\ndifferent parts such as text processing, term selection, term weighting and\nfinal classification. The accuracy of document classification is very\nimportant. Thus improvement in each part of classification should lead to\nbetter results and higher precision. Term weighting has a great impact on the\naccuracy of the classification. Most of the existing weighting methods exploit\nthe statistical information of terms in documents and do not consider semantic\nrelations between words. In this paper, an automated document classification\nsystem is presented that uses a novel term weighting method based on semantic\nrelations between terms. To evaluate the proposed method, three standard\nPersian corpuses are used. Experiment results show 2 to 4 percent improvement\nin classification accuracy compared with the best previous designed system for\nPersian documents."}
